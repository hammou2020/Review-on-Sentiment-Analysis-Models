{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6wQD_KG5lV2E"
   },
   "source": [
    "# Review of Sentiment Analysis Models\n",
    "\n",
    "The purpose of this review is to provide a broad but moderately deep understanding of different types of sentiment analysis models and their respective pros and cons. We will be exploring 3 different types of datasets and analysed them through different sentiment analysis models, which include:\n",
    "\n",
    "## 1. Traditional models\n",
    "\n",
    "a. TextBlob\n",
    "    \n",
    "b. VADER\n",
    "    \n",
    "c. Logistic Regression\n",
    "\n",
    "d. Naive Bayes \n",
    "\n",
    "e. Support Vector Machine (SVM)\n",
    "   \n",
    "    \n",
    "## 2. Deep learning models\n",
    "\n",
    "a. ULMFit\n",
    "\n",
    "b. BERT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_hmLWTDKomMl"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "CSjYPQ1Lw4JL",
    "outputId": "2bfbfbca-2a1d-4fc4-faba-5be3bc62db9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zfTybDq7mthZ"
   },
   "source": [
    "## Reading and Filtering Data\n",
    "### 1. Twitter dataset\n",
    "\n",
    "This dataset includes 1,600,000 tweets with emoticons pre-removed. The dataset was collected using the Twitter API. Source: https://www.kaggle.com/kazanova/sentiment140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 738
    },
    "colab_type": "code",
    "id": "CAtNBVaopq9v",
    "outputId": "35c5ae05-a3ba-4288-8a69-21f73c190701"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601966</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>AmandaMarie1028</td>\n",
       "      <td>Just woke up. Having no school is the best fee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601969</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>TheWDBoards</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601991</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>bpbabe</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602064</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>tinydiamondz</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602129</td>\n",
       "      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>RyanTrevMorris</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0  ...                                                  5\n",
       "0        0  ...  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
       "1        0  ...  is upset that he can't update his Facebook by ...\n",
       "2        0  ...  @Kenichan I dived many times for the ball. Man...\n",
       "3        0  ...    my whole body feels itchy and like its on fire \n",
       "4        0  ...  @nationwideclass no, it's not behaving at all....\n",
       "...     ..  ...                                                ...\n",
       "1599995  4  ...  Just woke up. Having no school is the best fee...\n",
       "1599996  4  ...  TheWDB.com - Very cool to hear old Walt interv...\n",
       "1599997  4  ...  Are you ready for your MoJo Makeover? Ask me f...\n",
       "1599998  4  ...  Happy 38th Birthday to my boo of alll time!!! ...\n",
       "1599999  4  ...  happy #charitytuesday @theNSPCC @SparksCharity...\n",
       "\n",
       "[1600000 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df = pd.read_csv('/content/gdrive/My Drive/Dataset/twitter_train.csv',\n",
    "                       encoding='iso-8859-1',\n",
    "                       header=None)\n",
    "tweet_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "colab_type": "code",
    "id": "VsF-51GNxRae",
    "outputId": "510a8287-42bf-41ea-d5de-e1225f6bfd3b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>Just woke up. Having no school is the best fee...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text polarity\n",
       "0        @switchfoot http://twitpic.com/2y1zl - Awww, t...      neg\n",
       "1        is upset that he can't update his Facebook by ...      neg\n",
       "2        @Kenichan I dived many times for the ball. Man...      neg\n",
       "3          my whole body feels itchy and like its on fire       neg\n",
       "4        @nationwideclass no, it's not behaving at all....      neg\n",
       "...                                                    ...      ...\n",
       "1599995  Just woke up. Having no school is the best fee...      pos\n",
       "1599996  TheWDB.com - Very cool to hear old Walt interv...      pos\n",
       "1599997  Are you ready for your MoJo Makeover? Ask me f...      pos\n",
       "1599998  Happy 38th Birthday to my boo of alll time!!! ...      pos\n",
       "1599999  happy #charitytuesday @theNSPCC @SparksCharity...      pos\n",
       "\n",
       "[200000 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df.columns = ['polarity', 'tweet ID', 'date', 'query', 'username', 'text']\n",
    "tweet_df = tweet_df.replace({'polarity':{0:'neg',4:'pos'}})\n",
    "tweet_df = tweet_df.filter(['text','polarity'])\n",
    "tweet_df_head = tweet_df.head(100000)\n",
    "tweet_df_tail = tweet_df.tail(100000)\n",
    "tweet_df = pd.concat([tweet_df_head, tweet_df_tail])\n",
    "tweet_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "id": "8LCo-GYaiNWH",
    "outputId": "8ee140d2-2ea4-4c60-cb1a-a76a23dec079"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 2 artists>"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAD4CAYAAAAZ1BptAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSJK6DAlJUtf/A9mZRcot5MSCAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(tweet_df['polarity'].unique(),tweet_df['polarity'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zhu0QdB_C3ZD"
   },
   "source": [
    "### 2. IMDB movie review dataset\n",
    "This dataset has 100,000 movie reviews which are being labelled as positive or negative review. Source: https://www.kaggle.com/utathya/imdb-review-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "colab_type": "code",
    "id": "gyyBCmF2JzVa",
    "outputId": "f7c09754-6ea8-4640-e715-04029fe9891e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>type</th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>Once again Mr. Costner has dragged out a movie...</td>\n",
       "      <td>neg</td>\n",
       "      <td>0_2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>This is an example of why the majority of acti...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10000_4.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>test</td>\n",
       "      <td>First of all I hate those moronic rappers, who...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10001_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>test</td>\n",
       "      <td>Not even the Beatles could write songs everyon...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10002_3.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>test</td>\n",
       "      <td>Brass pictures (movies is not a fitting word f...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10003_3.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>99995</td>\n",
       "      <td>train</td>\n",
       "      <td>Delightfully awful! Made by David Giancola, a ...</td>\n",
       "      <td>unsup</td>\n",
       "      <td>9998_0.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>99996</td>\n",
       "      <td>train</td>\n",
       "      <td>Watching Time Chasers, it obvious that it was ...</td>\n",
       "      <td>unsup</td>\n",
       "      <td>9999_0.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>99997</td>\n",
       "      <td>train</td>\n",
       "      <td>At the beginning we can see members of Troma t...</td>\n",
       "      <td>unsup</td>\n",
       "      <td>999_0.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>99998</td>\n",
       "      <td>train</td>\n",
       "      <td>The movie was incredible, ever since I saw it ...</td>\n",
       "      <td>unsup</td>\n",
       "      <td>99_0.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>99999</td>\n",
       "      <td>train</td>\n",
       "      <td>TCM came through by acquiring this wonderful, ...</td>\n",
       "      <td>unsup</td>\n",
       "      <td>9_0.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0   type  ...  label         file\n",
       "0               0   test  ...    neg      0_2.txt\n",
       "1               1   test  ...    neg  10000_4.txt\n",
       "2               2   test  ...    neg  10001_1.txt\n",
       "3               3   test  ...    neg  10002_3.txt\n",
       "4               4   test  ...    neg  10003_3.txt\n",
       "...           ...    ...  ...    ...          ...\n",
       "99995       99995  train  ...  unsup   9998_0.txt\n",
       "99996       99996  train  ...  unsup   9999_0.txt\n",
       "99997       99997  train  ...  unsup    999_0.txt\n",
       "99998       99998  train  ...  unsup     99_0.txt\n",
       "99999       99999  train  ...  unsup      9_0.txt\n",
       "\n",
       "[100000 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_df = pd.read_csv('/content/gdrive/My Drive/Dataset/imdb.csv', \n",
    "                      encoding='iso-8859-1')\n",
    "imdb_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "colab_type": "code",
    "id": "5DpllAv6KJV1",
    "outputId": "013623d2-f21b-4c82-9d97-44d371150ccc",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25000</th>\n",
       "      <td>Story of a man who has unnatural feelings for ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25001</th>\n",
       "      <td>Airport '77 starts as a brand new luxury 747 p...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25002</th>\n",
       "      <td>This film lacked something I couldn't put my f...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25003</th>\n",
       "      <td>Sorry everyone,,, I know this is supposed to b...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25004</th>\n",
       "      <td>When I was little my parents took me along to ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>Seeing as the vote average was pretty low, and...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>The plot had some wretched, unbelievable twist...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I am amazed at how this movie(and most others ...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>A Christmas Together actually came before my t...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>Working-class romantic drama from director Mar...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review label\n",
       "25000  Story of a man who has unnatural feelings for ...   neg\n",
       "25001  Airport '77 starts as a brand new luxury 747 p...   neg\n",
       "25002  This film lacked something I couldn't put my f...   neg\n",
       "25003  Sorry everyone,,, I know this is supposed to b...   neg\n",
       "25004  When I was little my parents took me along to ...   neg\n",
       "...                                                  ...   ...\n",
       "49995  Seeing as the vote average was pretty low, and...   pos\n",
       "49996  The plot had some wretched, unbelievable twist...   pos\n",
       "49997  I am amazed at how this movie(and most others ...   pos\n",
       "49998  A Christmas Together actually came before my t...   pos\n",
       "49999  Working-class romantic drama from director Mar...   pos\n",
       "\n",
       "[25000 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_df = imdb_df[(imdb_df['type'] == 'train') & (imdb_df['label'] != 'unsup')]\n",
    "imdb_df = imdb_df.filter(['review','label'])\n",
    "imdb_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "id": "COgsKnq-uyH3",
    "outputId": "8858310c-a5f3-48f4-cc96-d8df8b7adc6c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 2 artists>"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD4CAYAAAAO9oqkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ntfZWdEHhLAUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(imdb_df['label'].unique(),imdb_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nfnOQtSsQ-rt"
   },
   "source": [
    "### 3. Amazon's Products review dataset\n",
    "\n",
    "This is a list of over 160,000 consumer reviews for Amazon products like the Kindle, Fire TV Stick, and more provided by Datafiniti's Product Database. The dataset includes basic product information, rating and review text. Source: https://www.kaggle.com/harshaiitj08/amazon-product-ratings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "colab_type": "code",
    "id": "BRc1VmdpS2mb",
    "outputId": "e02a8b58-c6fa-4195-f1a4-5ff4712c5f39"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>reviews</th>\n",
       "      <th>ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>I like the item pricing. My granddaughter want...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Love the magnet easel... great for moving to d...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Both sides are magnetic.  A real plus when you...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Bought one a few years ago for my daughter and...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>I have a stainless steel refrigerator therefor...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167592</th>\n",
       "      <td>167592</td>\n",
       "      <td>This drone is very fun and super duarable. Its...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167593</th>\n",
       "      <td>167593</td>\n",
       "      <td>This is my brother's most prized toy. It's ext...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167594</th>\n",
       "      <td>167594</td>\n",
       "      <td>This Panther Drone toy is awesome. I definitel...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167595</th>\n",
       "      <td>167595</td>\n",
       "      <td>This is my first drone and it has proven to be...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167596</th>\n",
       "      <td>167596</td>\n",
       "      <td>This is a super fun toy to have around. In our...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>167597 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0                                            reviews  ratings\n",
       "0                0  I like the item pricing. My granddaughter want...      5.0\n",
       "1                1  Love the magnet easel... great for moving to d...      4.0\n",
       "2                2  Both sides are magnetic.  A real plus when you...      5.0\n",
       "3                3  Bought one a few years ago for my daughter and...      5.0\n",
       "4                4  I have a stainless steel refrigerator therefor...      4.0\n",
       "...            ...                                                ...      ...\n",
       "167592      167592  This drone is very fun and super duarable. Its...      5.0\n",
       "167593      167593  This is my brother's most prized toy. It's ext...      5.0\n",
       "167594      167594  This Panther Drone toy is awesome. I definitel...      5.0\n",
       "167595      167595  This is my first drone and it has proven to be...      5.0\n",
       "167596      167596  This is a super fun toy to have around. In our...      4.0\n",
       "\n",
       "[167597 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amz_df = pd.read_csv('/content/gdrive/My Drive/Dataset/amazon.csv')\n",
    "amz_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "colab_type": "code",
    "id": "8MGjjVgpJ7Q6",
    "outputId": "dc7c0695-f92e-4f65-ffc3-fea35251e833"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>A crappy cardboard ghost of the original.  Har...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>We have this same game but it was made in 1967...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>Hated this product.Predictable.  Not fun.  It ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>I had high hopes for this game, as I am a big ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>thought this was a book with pages to illustra...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6884</th>\n",
       "      <td>My 3 &amp; 4 y/o love this puzzle.  There is enoug...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6887</th>\n",
       "      <td>my 4 year old got this last year and still lov...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6888</th>\n",
       "      <td>This puzzle is very well made.  The pieces are...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6889</th>\n",
       "      <td>We love the Melissa and Doug line. We have abo...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6890</th>\n",
       "      <td>We bought this pirate ship puzzle along witht ...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22500 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                reviews  ratings\n",
       "157   A crappy cardboard ghost of the original.  Har...      1.0\n",
       "165   We have this same game but it was made in 1967...      1.0\n",
       "186   Hated this product.Predictable.  Not fun.  It ...      1.0\n",
       "191   I had high hopes for this game, as I am a big ...      1.0\n",
       "298   thought this was a book with pages to illustra...      1.0\n",
       "...                                                 ...      ...\n",
       "6884  My 3 & 4 y/o love this puzzle.  There is enoug...      5.0\n",
       "6887  my 4 year old got this last year and still lov...      5.0\n",
       "6888  This puzzle is very well made.  The pieces are...      5.0\n",
       "6889  We love the Melissa and Doug line. We have abo...      5.0\n",
       "6890  We bought this pirate ship puzzle along witht ...      5.0\n",
       "\n",
       "[22500 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amz_df = amz_df.dropna()\n",
    "amz1_df = amz_df.query(\"ratings == 1\").head(4500)\n",
    "amz2_df = amz_df.query(\"ratings == 2\").head(4500)\n",
    "amz3_df = amz_df.query(\"ratings == 3\").head(4500)\n",
    "amz4_df = amz_df.query(\"ratings == 4\").head(4500)\n",
    "amz5_df = amz_df.query(\"ratings == 5\").head(4500)\n",
    "amz_df = pd.concat([amz1_df, amz2_df, amz3_df, amz4_df, amz5_df])\n",
    "amz_df = amz_df.filter(['reviews','ratings'])\n",
    "amz_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "id": "Q5vtAi81UTuQ",
    "outputId": "47d98a78-3515-4d2c-d39d-3e34c7b8f498"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 5 artists>"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(amz_df['ratings'].unique(),amz_df['ratings'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Sn39hhPqdoMv"
   },
   "source": [
    "## Cleaning and Pre-processing Data \n",
    "### A) Remove punctuation, number & lowercase the text\n",
    "### 1. Twitter dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 639
    },
    "colab_type": "code",
    "id": "8or5J3EwyEsI",
    "outputId": "0b1167f8-0a42-4222-b4f0-598db7cad296"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>switchfoot httptwitpiccomyzl  awww thats a bum...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is upset that he cant update his facebook by t...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kenichan i dived many times for the ball manag...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nationwideclass no its not behaving at all im ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>kwesidei not the whole crew</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>need a hug</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>loltrish hey  long time no see yes rains a bit...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>tatianak nope they didnt have it</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>twittera que me muera</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>spring break in plain city its snowing</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>i just repierced my ears</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>caregiving i couldnt bear to watch it  and i t...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>octolinz it it counts idk why i did either you...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>smarrison i wouldve been the first but i didnt...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>iamjazzyfizzle i wish i got to watch it with y...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>hollis death scene will hurt me severely to wa...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>about to file taxes</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>lettya ahh ive always wanted to see rent  love...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>fakerpattypattz oh dear were you drinking out ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text polarity\n",
       "0   switchfoot httptwitpiccomyzl  awww thats a bum...      neg\n",
       "1   is upset that he cant update his facebook by t...      neg\n",
       "2   kenichan i dived many times for the ball manag...      neg\n",
       "3     my whole body feels itchy and like its on fire       neg\n",
       "4   nationwideclass no its not behaving at all im ...      neg\n",
       "5                        kwesidei not the whole crew       neg\n",
       "6                                         need a hug       neg\n",
       "7   loltrish hey  long time no see yes rains a bit...      neg\n",
       "8                   tatianak nope they didnt have it       neg\n",
       "9                             twittera que me muera        neg\n",
       "10            spring break in plain city its snowing       neg\n",
       "11                          i just repierced my ears       neg\n",
       "12  caregiving i couldnt bear to watch it  and i t...      neg\n",
       "13  octolinz it it counts idk why i did either you...      neg\n",
       "14  smarrison i wouldve been the first but i didnt...      neg\n",
       "15  iamjazzyfizzle i wish i got to watch it with y...      neg\n",
       "16  hollis death scene will hurt me severely to wa...      neg\n",
       "17                               about to file taxes       neg\n",
       "18  lettya ahh ive always wanted to see rent  love...      neg\n",
       "19  fakerpattypattz oh dear were you drinking out ...      neg"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean(text):\n",
    "    text = text.lower()\n",
    "    text = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    text = re.sub('[0-9]+', '', text)\n",
    "    return text\n",
    "\n",
    "tweet_df['text'] = tweet_df['text'].apply(lambda x: clean(x))\n",
    "tweet_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "otcp2aFMMGZO"
   },
   "source": [
    "### 2. IMDB movie review dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 639
    },
    "colab_type": "code",
    "id": "uTXqbqXjKaUN",
    "outputId": "629d83d1-733c-40f3-b057-efcc9727ade5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25000</th>\n",
       "      <td>story of a man who has unnatural feelings for ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25001</th>\n",
       "      <td>airport  starts as a brand new luxury  plane i...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25002</th>\n",
       "      <td>this film lacked something i couldnt put my fi...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25003</th>\n",
       "      <td>sorry everyone i know this is supposed to be a...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25004</th>\n",
       "      <td>when i was little my parents took me along to ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25005</th>\n",
       "      <td>it appears that many critics find the idea of ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25006</th>\n",
       "      <td>the second attempt by a new york intellectual ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25007</th>\n",
       "      <td>i dont know who to blame the timid writers or ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25008</th>\n",
       "      <td>this film is mediocre at best angie harmon is ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25009</th>\n",
       "      <td>the film is bad there is no other way to say i...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25010</th>\n",
       "      <td>this film is one giant pant load paul schrader...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25011</th>\n",
       "      <td>the plot for descent if it actually can be cal...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25012</th>\n",
       "      <td>plot is not worth discussion even if it hints ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25013</th>\n",
       "      <td>this film is about a male escort getting invol...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25014</th>\n",
       "      <td>this movie must be in line for the most boring...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25015</th>\n",
       "      <td>a wornout plot of a man who takes the rap for ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25016</th>\n",
       "      <td>i saw this movie at a drivein in  until howard...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25017</th>\n",
       "      <td>ghost of dragstrip hollow is a typical s teens...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25018</th>\n",
       "      <td>ghost of dragstrip hollow was one of the many ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25019</th>\n",
       "      <td>ghost of dragstrip hollow appears to take plac...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review label\n",
       "25000  story of a man who has unnatural feelings for ...   neg\n",
       "25001  airport  starts as a brand new luxury  plane i...   neg\n",
       "25002  this film lacked something i couldnt put my fi...   neg\n",
       "25003  sorry everyone i know this is supposed to be a...   neg\n",
       "25004  when i was little my parents took me along to ...   neg\n",
       "25005  it appears that many critics find the idea of ...   neg\n",
       "25006  the second attempt by a new york intellectual ...   neg\n",
       "25007  i dont know who to blame the timid writers or ...   neg\n",
       "25008  this film is mediocre at best angie harmon is ...   neg\n",
       "25009  the film is bad there is no other way to say i...   neg\n",
       "25010  this film is one giant pant load paul schrader...   neg\n",
       "25011  the plot for descent if it actually can be cal...   neg\n",
       "25012  plot is not worth discussion even if it hints ...   neg\n",
       "25013  this film is about a male escort getting invol...   neg\n",
       "25014  this movie must be in line for the most boring...   neg\n",
       "25015  a wornout plot of a man who takes the rap for ...   neg\n",
       "25016  i saw this movie at a drivein in  until howard...   neg\n",
       "25017  ghost of dragstrip hollow is a typical s teens...   neg\n",
       "25018  ghost of dragstrip hollow was one of the many ...   neg\n",
       "25019  ghost of dragstrip hollow appears to take plac...   neg"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean(text):\n",
    "    text = text.lower()\n",
    "    text = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    text = re.sub('[0-9]+', '', text)\n",
    "    return text\n",
    "\n",
    "imdb_df['review'] = imdb_df['review'].apply(lambda x: clean(x))\n",
    "imdb_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HnimTVnkeqc7"
   },
   "source": [
    "### 3. Amazon's Product review dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 639
    },
    "colab_type": "code",
    "id": "qmclcOIgTtuF",
    "outputId": "44ca7674-1451-4ba1-dcf3-2c5fd8248c23",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>a crappy cardboard ghost of the original  hard...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>we have this same game but it was made in  we ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>hated this productpredictable  not fun  it att...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>i had high hopes for this game as i am a big f...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>thought this was a book with pages to illustra...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>started out as a great road trip activity for ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534</th>\n",
       "      <td>it didnt work when we bought it a bummer becau...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586</th>\n",
       "      <td>heres a creepy little elf and a book  if you w...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>this game looked like fun but after trying it ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>742</th>\n",
       "      <td>it took forever for me to write this review si...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>my granddaughter abigail had to have the new a...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>941</th>\n",
       "      <td>i thought my one year old would be super into ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1017</th>\n",
       "      <td>this book contains lead do not give it to your...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1123</th>\n",
       "      <td>first of all i always wanted to play this game...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1256</th>\n",
       "      <td>other than being very pretty i dont get why th...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1344</th>\n",
       "      <td>i installed fresh batteries in the robot and s...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1409</th>\n",
       "      <td>again like arkham horror i really looked forwa...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1469</th>\n",
       "      <td>so this is a star wars game if you like star w...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1521</th>\n",
       "      <td>i thought these were made of metal they are a ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1565</th>\n",
       "      <td>the only reason that they can even justify sel...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                reviews  ratings\n",
       "157   a crappy cardboard ghost of the original  hard...      1.0\n",
       "165   we have this same game but it was made in  we ...      1.0\n",
       "186   hated this productpredictable  not fun  it att...      1.0\n",
       "191   i had high hopes for this game as i am a big f...      1.0\n",
       "298   thought this was a book with pages to illustra...      1.0\n",
       "330   started out as a great road trip activity for ...      1.0\n",
       "534   it didnt work when we bought it a bummer becau...      1.0\n",
       "586   heres a creepy little elf and a book  if you w...      1.0\n",
       "730   this game looked like fun but after trying it ...      1.0\n",
       "742   it took forever for me to write this review si...      1.0\n",
       "871   my granddaughter abigail had to have the new a...      1.0\n",
       "941   i thought my one year old would be super into ...      1.0\n",
       "1017  this book contains lead do not give it to your...      1.0\n",
       "1123  first of all i always wanted to play this game...      1.0\n",
       "1256  other than being very pretty i dont get why th...      1.0\n",
       "1344  i installed fresh batteries in the robot and s...      1.0\n",
       "1409  again like arkham horror i really looked forwa...      1.0\n",
       "1469  so this is a star wars game if you like star w...      1.0\n",
       "1521  i thought these were made of metal they are a ...      1.0\n",
       "1565  the only reason that they can even justify sel...      1.0"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean(text):\n",
    "    text = text.lower()\n",
    "    text = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    text = re.sub('[0-9]+', '', text)\n",
    "    return text\n",
    "\n",
    "amz_df['reviews'] = amz_df['reviews'].apply(lambda x: clean(x))\n",
    "amz_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p9VQ_NcFyXHj"
   },
   "source": [
    "## B) Count vectorizer\n",
    "The vectorizer shows the frequecy of a term t occurs in a document d. \n",
    "### 1. Twitter dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 422
    },
    "colab_type": "code",
    "id": "h5U5X35jybUk",
    "outputId": "5c905307-749f-4e28-d0a1-7d3592355667"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day</th>\n",
       "      <th>dont</th>\n",
       "      <th>going</th>\n",
       "      <th>good</th>\n",
       "      <th>got</th>\n",
       "      <th>home</th>\n",
       "      <th>im</th>\n",
       "      <th>just</th>\n",
       "      <th>know</th>\n",
       "      <th>like</th>\n",
       "      <th>lol</th>\n",
       "      <th>love</th>\n",
       "      <th>new</th>\n",
       "      <th>night</th>\n",
       "      <th>really</th>\n",
       "      <th>think</th>\n",
       "      <th>time</th>\n",
       "      <th>today</th>\n",
       "      <th>want</th>\n",
       "      <th>work</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199995</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199996</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199997</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199998</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199999</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200000 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        day  dont  going  good  got  ...  think  time  today  want  work\n",
       "0         1     0      0     0    1  ...      0     0      0     0     0\n",
       "1         0     0      0     0    0  ...      0     0      1     0     0\n",
       "2         0     0      0     0    0  ...      0     0      0     0     0\n",
       "3         0     0      0     0    0  ...      0     0      0     0     0\n",
       "4         0     0      0     0    0  ...      0     0      0     0     0\n",
       "...     ...   ...    ...   ...  ...  ...    ...   ...    ...   ...   ...\n",
       "199995    0     0      0     0    0  ...      0     0      0     0     0\n",
       "199996    0     0      0     0    0  ...      0     0      0     0     0\n",
       "199997    0     0      0     0    0  ...      0     0      0     0     0\n",
       "199998    0     0      0     0    0  ...      0     1      0     0     0\n",
       "199999    0     0      0     0    0  ...      0     0      0     0     0\n",
       "\n",
       "[200000 rows x 20 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1,2), \n",
    "                             stop_words=\"english\", \n",
    "                             max_features=20)\n",
    "vector = vectorizer.fit_transform(tweet_df[\"text\"]).toarray()\n",
    "tweet_count_vector_df = pd.DataFrame(vector, columns = vectorizer.get_feature_names())\n",
    "tweet_count_vector_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gKXpgPVLd5ef"
   },
   "source": [
    "###  2. IMDB movie review dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 438
    },
    "colab_type": "code",
    "id": "a2qPCRFEKkH8",
    "outputId": "a43c31cf-56a0-4a81-b7be-103f28c635c6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bad</th>\n",
       "      <th>br</th>\n",
       "      <th>br br</th>\n",
       "      <th>characters</th>\n",
       "      <th>dont</th>\n",
       "      <th>film</th>\n",
       "      <th>films</th>\n",
       "      <th>good</th>\n",
       "      <th>great</th>\n",
       "      <th>just</th>\n",
       "      <th>like</th>\n",
       "      <th>make</th>\n",
       "      <th>movie</th>\n",
       "      <th>movies</th>\n",
       "      <th>people</th>\n",
       "      <th>really</th>\n",
       "      <th>story</th>\n",
       "      <th>think</th>\n",
       "      <th>time</th>\n",
       "      <th>way</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       bad  br  br br  characters  dont  ...  really  story  think  time  way\n",
       "0        0   0      0           0     0  ...       0      1      1     1    0\n",
       "1        1   4      0           2     1  ...       0      0      1     4    0\n",
       "2        0   2      0           0     2  ...       0      0      0     0    0\n",
       "3        0   0      0           0     0  ...       1      1      1     1    0\n",
       "4        0   0      0           2     0  ...       0      1      0     0    0\n",
       "...    ...  ..    ...         ...   ...  ...     ...    ...    ...   ...  ...\n",
       "24995    0   3      0           0     0  ...       0      0      1     0    0\n",
       "24996    0   1      0           0     0  ...       0      0      0     0    0\n",
       "24997    1   5      0           0     1  ...       0      1      0     0    1\n",
       "24998    0   0      0           0     1  ...       0      0      0     1    0\n",
       "24999    0   0      0           2     0  ...       1      1      0     0    1\n",
       "\n",
       "[25000 rows x 20 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1,2), \n",
    "                             stop_words=\"english\",\n",
    "                             max_features=20)\n",
    "vector = vectorizer.fit_transform(imdb_df[\"review\"]).toarray()\n",
    "imdb_count_vector_df = pd.DataFrame(vector, columns = vectorizer.get_feature_names())\n",
    "imdb_count_vector_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G1TpiMjie0xE"
   },
   "source": [
    "### 3. Amazon's Products review dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 438
    },
    "colab_type": "code",
    "id": "0oIwW1ZKbZsM",
    "outputId": "bddd157f-55be-4352-c50d-55a99b6a31d2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>daughter</th>\n",
       "      <th>dont</th>\n",
       "      <th>fun</th>\n",
       "      <th>game</th>\n",
       "      <th>good</th>\n",
       "      <th>got</th>\n",
       "      <th>great</th>\n",
       "      <th>just</th>\n",
       "      <th>kids</th>\n",
       "      <th>like</th>\n",
       "      <th>little</th>\n",
       "      <th>old</th>\n",
       "      <th>play</th>\n",
       "      <th>really</th>\n",
       "      <th>son</th>\n",
       "      <th>time</th>\n",
       "      <th>toy</th>\n",
       "      <th>use</th>\n",
       "      <th>year</th>\n",
       "      <th>year old</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22495</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22496</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22497</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22498</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22499</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22500 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       daughter  dont  fun  game  good  ...  time  toy  use  year  year old\n",
       "0             0     0    0     0     0  ...     0    0    0     0         0\n",
       "1             0     0    0     2     0  ...     0    0    0     0         0\n",
       "2             0     0    1     1     0  ...     0    0    0     0         0\n",
       "3             0     0    0     8     0  ...     0    0    1     0         0\n",
       "4             0     0    0     0     0  ...     0    0    0     0         0\n",
       "...         ...   ...  ...   ...   ...  ...   ...  ...  ...   ...       ...\n",
       "22495         0     0    0     0     0  ...     0    0    0     0         0\n",
       "22496         0     0    0     0     0  ...     0    0    0     2         1\n",
       "22497         0     0    0     0     0  ...     0    0    0     1         1\n",
       "22498         0     0    0     0     0  ...     0    0    0     1         0\n",
       "22499         0     0    0     0     1  ...     0    0    0     2         2\n",
       "\n",
       "[22500 rows x 20 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1,2), \n",
    "                             stop_words=\"english\", \n",
    "                             max_features=20)\n",
    "vector = vectorizer.fit_transform(amz_df[\"reviews\"]).toarray()\n",
    "amz_count_vector_df = pd.DataFrame(vector, columns = vectorizer.get_feature_names())\n",
    "amz_count_vector_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bJTX-zXjyfg_"
   },
   "source": [
    "## D) TF-IDF vectorizer\n",
    "\n",
    "1. Term Frequency (TF)\n",
    "\n",
    "  The frequency of a word **w** appears in a document divided by the total number of words in the document. Every document has its own term frequency.\n",
    "\n",
    "  ![alt text](https://miro.medium.com/proxy/1*HM0Vcdrx2RApOyjp_ZeW_Q.png)\n",
    "\n",
    "2. Inverse Data Frequency (IDF)\n",
    "\n",
    "  The log of the number of documents divided by the number of documents that  contain the word **w**. IDF determines the weight of rare words across all documents in the corpus.\n",
    "\n",
    "  ![alt text](https://miro.medium.com/proxy/1*A5YGwFpcTd0YTCdgoiHFUw.png)\n",
    "\n",
    "  Eg: Assume we have 1000 documents and the word 'the' appears 1000 times while 'python' appears 500 times across all the documents and the IDF score will penalise the common words.\n",
    "  \n",
    "  idf(the) = log(1000/1000) = 0\n",
    "  \n",
    "  idf(python) = log(1000/500) = 0.301\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "  The TF-IDF vectorizer is simply the TF multiplied by IDF. The TF-IDF score increases with number of occurrences within a document and rarity of terms in the collection of documents.\n",
    "\n",
    "![alt text](https://miro.medium.com/proxy/1*nSqHXwOIJ2fa_EFLTh5KYw.png)\n",
    "\n",
    "Reference: https://towardsdatascience.com/natural-language-processing-feature-engineering-using-tf-idf-e8b9d00e7e76\n",
    "\n",
    "### 1. Twitter dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 422
    },
    "colab_type": "code",
    "id": "4IOIZ2uTyjTg",
    "outputId": "4d701dd1-6f60-4adb-8ac9-e8de9eef1367"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day</th>\n",
       "      <th>dont</th>\n",
       "      <th>going</th>\n",
       "      <th>good</th>\n",
       "      <th>got</th>\n",
       "      <th>home</th>\n",
       "      <th>im</th>\n",
       "      <th>just</th>\n",
       "      <th>know</th>\n",
       "      <th>like</th>\n",
       "      <th>lol</th>\n",
       "      <th>love</th>\n",
       "      <th>new</th>\n",
       "      <th>night</th>\n",
       "      <th>really</th>\n",
       "      <th>think</th>\n",
       "      <th>time</th>\n",
       "      <th>today</th>\n",
       "      <th>want</th>\n",
       "      <th>work</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.671002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.741456</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199995</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199996</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199997</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199998</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199999</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200000 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             day  dont  going  good       got  ...  think  time  today  want  work\n",
       "0       0.671002   0.0    0.0   0.0  0.741456  ...    0.0   0.0    0.0   0.0   0.0\n",
       "1       0.000000   0.0    0.0   0.0  0.000000  ...    0.0   0.0    1.0   0.0   0.0\n",
       "2       0.000000   0.0    0.0   0.0  0.000000  ...    0.0   0.0    0.0   0.0   0.0\n",
       "3       0.000000   0.0    0.0   0.0  0.000000  ...    0.0   0.0    0.0   0.0   0.0\n",
       "4       0.000000   0.0    0.0   0.0  0.000000  ...    0.0   0.0    0.0   0.0   0.0\n",
       "...          ...   ...    ...   ...       ...  ...    ...   ...    ...   ...   ...\n",
       "199995  0.000000   0.0    0.0   0.0  0.000000  ...    0.0   0.0    0.0   0.0   0.0\n",
       "199996  0.000000   0.0    0.0   0.0  0.000000  ...    0.0   0.0    0.0   0.0   0.0\n",
       "199997  0.000000   0.0    0.0   0.0  0.000000  ...    0.0   0.0    0.0   0.0   0.0\n",
       "199998  0.000000   0.0    0.0   0.0  0.000000  ...    0.0   1.0    0.0   0.0   0.0\n",
       "199999  0.000000   0.0    0.0   0.0  0.000000  ...    0.0   0.0    0.0   0.0   0.0\n",
       "\n",
       "[200000 rows x 20 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2), \n",
    "                             stop_words=\"english\", \n",
    "                             max_features=20)\n",
    "vector = vectorizer.fit_transform(tweet_df['text']).toarray()\n",
    "tweet_tfidf_vector_df = pd.DataFrame(vector, columns = vectorizer.get_feature_names())\n",
    "tweet_tfidf_vector_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "16mRTl8Ld9-R"
   },
   "source": [
    "### 2. IMDB movie review dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 438
    },
    "colab_type": "code",
    "id": "ZxFkecnHKpP7",
    "outputId": "270013b4-480e-4d2b-81be-11fb4f0f83f4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bad</th>\n",
       "      <th>br</th>\n",
       "      <th>br br</th>\n",
       "      <th>characters</th>\n",
       "      <th>dont</th>\n",
       "      <th>film</th>\n",
       "      <th>films</th>\n",
       "      <th>good</th>\n",
       "      <th>great</th>\n",
       "      <th>just</th>\n",
       "      <th>like</th>\n",
       "      <th>make</th>\n",
       "      <th>movie</th>\n",
       "      <th>movies</th>\n",
       "      <th>people</th>\n",
       "      <th>really</th>\n",
       "      <th>story</th>\n",
       "      <th>think</th>\n",
       "      <th>time</th>\n",
       "      <th>way</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.335162</td>\n",
       "      <td>0.405050</td>\n",
       "      <td>0.316614</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.412560</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.375876</td>\n",
       "      <td>0.429411</td>\n",
       "      <td>0.357035</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.130018</td>\n",
       "      <td>0.322704</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.270551</td>\n",
       "      <td>0.125055</td>\n",
       "      <td>0.254427</td>\n",
       "      <td>0.410294</td>\n",
       "      <td>0.104034</td>\n",
       "      <td>0.377180</td>\n",
       "      <td>0.393106</td>\n",
       "      <td>0.187491</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.133288</td>\n",
       "      <td>0.443291</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.430096</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666688</td>\n",
       "      <td>0.226064</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.523926</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.211993</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.326423</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.378259</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.734474</td>\n",
       "      <td>0.221856</td>\n",
       "      <td>0.224530</td>\n",
       "      <td>0.256509</td>\n",
       "      <td>0.213275</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.394546</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.618386</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.573268</td>\n",
       "      <td>0.136709</td>\n",
       "      <td>0.186747</td>\n",
       "      <td>0.115979</td>\n",
       "      <td>0.193218</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.170142</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707811</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.367688</td>\n",
       "      <td>0.287410</td>\n",
       "      <td>0.274159</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.232585</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.389802</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.634500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.772923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>0.128307</td>\n",
       "      <td>0.398070</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.123409</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.102664</td>\n",
       "      <td>0.124072</td>\n",
       "      <td>0.096983</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.549381</td>\n",
       "      <td>0.653754</td>\n",
       "      <td>0.125542</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.115136</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.127580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.652705</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.489289</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.578422</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.710070</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.358943</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.257930</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.302561</td>\n",
       "      <td>0.306207</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.339303</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            bad        br  br br  ...     think      time       way\n",
       "0      0.000000  0.000000    0.0  ...  0.429411  0.357035  0.000000\n",
       "1      0.130018  0.322704    0.0  ...  0.133288  0.443291  0.000000\n",
       "2      0.000000  0.430096    0.0  ...  0.000000  0.000000  0.000000\n",
       "3      0.000000  0.000000    0.0  ...  0.256509  0.213275  0.000000\n",
       "4      0.000000  0.000000    0.0  ...  0.000000  0.000000  0.000000\n",
       "...         ...       ...    ...  ...       ...       ...       ...\n",
       "24995  0.000000  0.707811    0.0  ...  0.389802  0.000000  0.000000\n",
       "24996  0.000000  0.634500    0.0  ...  0.000000  0.000000  0.000000\n",
       "24997  0.128307  0.398070    0.0  ...  0.000000  0.000000  0.127580\n",
       "24998  0.000000  0.000000    0.0  ...  0.000000  0.578422  0.000000\n",
       "24999  0.000000  0.000000    0.0  ...  0.000000  0.000000  0.339303\n",
       "\n",
       "[25000 rows x 20 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1,2), \n",
    "                             stop_words=\"english\", \n",
    "                             max_features=20)\n",
    "vector = vectorizer.fit_transform(imdb_df['review']).toarray()\n",
    "imdb_tfidf_vector_df = pd.DataFrame(vector, columns = vectorizer.get_feature_names())\n",
    "imdb_tfidf_vector_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sZ844o2Ne4iG"
   },
   "source": [
    "### 3. Amazon's Products review dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 438
    },
    "colab_type": "code",
    "id": "6u5WBrNUbi66",
    "outputId": "a299374a-db12-471b-def8-a0d0781a9114"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>daughter</th>\n",
       "      <th>dont</th>\n",
       "      <th>fun</th>\n",
       "      <th>game</th>\n",
       "      <th>good</th>\n",
       "      <th>got</th>\n",
       "      <th>great</th>\n",
       "      <th>just</th>\n",
       "      <th>kids</th>\n",
       "      <th>like</th>\n",
       "      <th>little</th>\n",
       "      <th>old</th>\n",
       "      <th>play</th>\n",
       "      <th>really</th>\n",
       "      <th>son</th>\n",
       "      <th>time</th>\n",
       "      <th>toy</th>\n",
       "      <th>use</th>\n",
       "      <th>year</th>\n",
       "      <th>year old</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.586391</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.487461</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.253213</td>\n",
       "      <td>0.283729</td>\n",
       "      <td>0.523363</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372708</td>\n",
       "      <td>0.395595</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.328855</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.683298</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.359934</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.948393</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.098549</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.102383</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.107862</td>\n",
       "      <td>0.226104</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.132661</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.648411</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.76129</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22495</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22496</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.39742</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.328740</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.753432</td>\n",
       "      <td>0.407833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22497</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.423438</td>\n",
       "      <td>0.358947</td>\n",
       "      <td>0.402205</td>\n",
       "      <td>0.370951</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.425087</td>\n",
       "      <td>0.460200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22498</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.524964</td>\n",
       "      <td>0.445010</td>\n",
       "      <td>0.498640</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.527008</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22499</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.279224</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.229821</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.475014</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.544337</td>\n",
       "      <td>0.589300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22500 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       daughter  dont       fun      game  ...  toy       use      year  year old\n",
       "0           0.0   0.0  0.000000  0.000000  ...  0.0  0.000000  0.000000  0.000000\n",
       "1           0.0   0.0  0.000000  0.586391  ...  0.0  0.000000  0.000000  0.000000\n",
       "2           0.0   0.0  0.372708  0.395595  ...  0.0  0.000000  0.000000  0.000000\n",
       "3           0.0   0.0  0.000000  0.948393  ...  0.0  0.132661  0.000000  0.000000\n",
       "4           0.0   0.0  0.000000  0.000000  ...  0.0  0.000000  0.000000  0.000000\n",
       "...         ...   ...       ...       ...  ...  ...       ...       ...       ...\n",
       "22495       0.0   0.0  0.000000  0.000000  ...  0.0  0.000000  0.000000  0.000000\n",
       "22496       0.0   0.0  0.000000  0.000000  ...  0.0  0.000000  0.753432  0.407833\n",
       "22497       0.0   0.0  0.000000  0.000000  ...  0.0  0.000000  0.425087  0.460200\n",
       "22498       0.0   0.0  0.000000  0.000000  ...  0.0  0.000000  0.527008  0.000000\n",
       "22499       0.0   0.0  0.000000  0.000000  ...  0.0  0.000000  0.544337  0.589300\n",
       "\n",
       "[22500 rows x 20 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1,2), \n",
    "                             stop_words=\"english\", \n",
    "                             max_features=20)\n",
    "vector = vectorizer.fit_transform(amz_df['reviews']).toarray()\n",
    "amz_tfidf_vector_df = pd.DataFrame(vector, columns = vectorizer.get_feature_names())\n",
    "amz_tfidf_vector_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o2K9NcSSyo9j"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# TextBlob\n",
    "\n",
    "TextBlob is built based on NLTK and Pattern. It has great API for all the common NLP operations. It’s great for initial prototyping in almost every NLP project. Unfortunately, it inherits the low performance from NLTK and therefore it’s not good for large scale production usage. \n",
    "\n",
    "\n",
    "### Functionalities:\n",
    "\n",
    "1. Tokenization\n",
    "2. Parts Of Speech Tagging (POS) \n",
    "3. Name Entity Recognition (NER)\n",
    "4. Classification \n",
    "5. Sentiment analysis\n",
    "\n",
    "Sentiment analysis using TextBlob will result two sentiment metrics, which are: \n",
    "1. Polarity \n",
    "\n",
    "    lies in the range of [-1,1] where 1 means positive statement and -1 means a negative statement    \n",
    "2. Subjectivity \n",
    "\n",
    "    lies in the range of [0,1] where 0 indicates as factual information ans 1 indicates as personal opinion\n",
    "\n",
    "Example: \n",
    "\n",
    "* Towards Data Science is a great platform to learn data science.\n",
    "    \n",
    "    Sentiment(polarity=0.8, subjectivity=0.75)\n",
    "\n",
    "* We can see that polarity is 0.8, which means that the statement is positive and 0.75 subjectivity refers that mostly it is a personal opinion and not a factual information. We can say it's a positive and subjective statement.\n",
    "\n",
    "### Pros:\n",
    "\n",
    "1. easy to learn and offers a lot of features like sentiment analysis, pos-tagging, noun phrase extraction etc\n",
    "\n",
    "2. provides language translation and detection which is powered by Google Translate\n",
    "\n",
    "3. does not require training process to predict sentiment of text, more efficient in some extent\n",
    "\n",
    "### Cons:\n",
    "\n",
    "1. slow in performance when dealing with huge amount of data (more than 100k)\n",
    "\n",
    "2. does not recognise the context of data, hence the result generated are usually less accurate compared to other custom trained model (Eg: Logistic Regression, Naive Bayes etc)\n",
    "\n",
    "For more details, kindly refer to Textblob documentation https://textblob.readthedocs.io/en/dev/\n",
    "\n",
    "Reference: https://www.analyticsvidhya.com/blog/2018/02/natural-language-processing-for-beginners-using-textblob/) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "eFgCARtqfPzI",
    "outputId": "35dd2ced-8668-4246-b80f-87e9b092c172"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"what are you doing?\")"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Example for Textblob language translation\n",
    "from textblob import TextBlob\n",
    "TextBlob('你在做什么？').translate(from_lang='zh-CN', to ='en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aB9fO4nkgE6E"
   },
   "source": [
    "### 1. Twitter dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 858
    },
    "colab_type": "code",
    "id": "uO5Q0qJiytog",
    "outputId": "c63f27cc-864e-48a1-ba82-da0c2203abc3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>polarity</th>\n",
       "      <th>textblob_polarity</th>\n",
       "      <th>textblob_predicted</th>\n",
       "      <th>textblob_subjectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>switchfoot httptwitpiccomyzl  awww thats a bum...</td>\n",
       "      <td>neg</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.4500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is upset that he cant update his facebook by t...</td>\n",
       "      <td>neg</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kenichan i dived many times for the ball manag...</td>\n",
       "      <td>neg</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>neg</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.4000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nationwideclass no its not behaving at all im ...</td>\n",
       "      <td>neg</td>\n",
       "      <td>-0.6250</td>\n",
       "      <td>neg</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>just woke up having no school is the best feel...</td>\n",
       "      <td>pos</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.3000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>thewdbcom  very cool to hear old walt intervie...</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.2775</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.5225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>are you ready for your mojo makeover ask me fo...</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>happy th birthday to my boo of alll time tupac...</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.8000</td>\n",
       "      <td>pos</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>happy charitytuesday thenspcc sparkscharity sp...</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.8000</td>\n",
       "      <td>pos</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text  ... textblob_subjectivity\n",
       "0        switchfoot httptwitpiccomyzl  awww thats a bum...  ...                0.4500\n",
       "1        is upset that he cant update his facebook by t...  ...                0.0000\n",
       "2        kenichan i dived many times for the ball manag...  ...                0.5000\n",
       "3          my whole body feels itchy and like its on fire   ...                0.4000\n",
       "4        nationwideclass no its not behaving at all im ...  ...                1.0000\n",
       "...                                                    ...  ...                   ...\n",
       "1599995  just woke up having no school is the best feel...  ...                0.3000\n",
       "1599996  thewdbcom  very cool to hear old walt intervie...  ...                0.5225\n",
       "1599997  are you ready for your mojo makeover ask me fo...  ...                0.5000\n",
       "1599998  happy th birthday to my boo of alll time tupac...  ...                1.0000\n",
       "1599999  happy charitytuesday thenspcc sparkscharity sp...  ...                1.0000\n",
       "\n",
       "[200000 rows x 5 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df['textblob_polarity'] = tweet_df.apply(lambda x: TextBlob(x['text']).sentiment.polarity, axis=1)\n",
    "\n",
    "i = 0\n",
    "predicted_value = [ ] \n",
    "while i < len(tweet_df):\n",
    "    if ((tweet_df.iloc[i]['textblob_polarity'] >= 0)):\n",
    "        predicted_value.append('pos')\n",
    "        i = i+1\n",
    "    elif ((tweet_df.iloc[i]['textblob_polarity'] < 0)):\n",
    "        predicted_value.append('neg')\n",
    "        i = i+1\n",
    "tweet_df['textblob_predicted'] = predicted_value\n",
    "\n",
    "tweet_df['textblob_subjectivity'] = tweet_df.apply(lambda x: TextBlob(x['text']).sentiment.subjectivity, axis=1)       \n",
    "tweet_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "cMBgrlCq4EWr",
    "outputId": "529acbba-78e8-48e6-80fa-0acf8c95ed4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score      : 0.6920631466036549\n",
      "Accuracy score: 0.60383\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score,accuracy_score\n",
    "m1 = f1_score(tweet_df['polarity'], tweet_df['textblob_predicted'], pos_label='pos')\n",
    "m2 = accuracy_score(tweet_df['polarity'], tweet_df['textblob_predicted'])\n",
    "print('F1 score      :',m1)\n",
    "print('Accuracy score:',m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dB_NF6H3eC6S"
   },
   "source": [
    "We can observe that TextBlob doesn't work well with Twitter dataset as the F1 score and accuracy score are 0.692 and 0.604 respectively, which are belowe 0.7. This shows results generated by TextBlob are less in terms of accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. IMDB movie review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 990
    },
    "colab_type": "code",
    "id": "T1KYHaFGK_Ok",
    "outputId": "993de994-3387-4343-d9fb-24fb9101edcf"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "      <th>textblob_polarity</th>\n",
       "      <th>textblob_predicted</th>\n",
       "      <th>textblob_subjectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25000</th>\n",
       "      <td>story of a man who has unnatural feelings for ...</td>\n",
       "      <td>neg</td>\n",
       "      <td>-0.071759</td>\n",
       "      <td>neg</td>\n",
       "      <td>0.620370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25001</th>\n",
       "      <td>airport  starts as a brand new luxury  plane i...</td>\n",
       "      <td>neg</td>\n",
       "      <td>0.040492</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.499230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25002</th>\n",
       "      <td>this film lacked something i couldnt put my fi...</td>\n",
       "      <td>neg</td>\n",
       "      <td>0.079167</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.527778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25003</th>\n",
       "      <td>sorry everyone i know this is supposed to be a...</td>\n",
       "      <td>neg</td>\n",
       "      <td>0.043542</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.647083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25004</th>\n",
       "      <td>when i was little my parents took me along to ...</td>\n",
       "      <td>neg</td>\n",
       "      <td>-0.055741</td>\n",
       "      <td>neg</td>\n",
       "      <td>0.557328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>seeing as the vote average was pretty low and ...</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.291961</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.563529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>the plot had some wretched unbelievable twists...</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>i am amazed at how this movieand most others h...</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.136099</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.645995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>a christmas together actually came before my t...</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.118069</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.461614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>workingclass romantic drama from director mart...</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.121951</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.635819</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  ... textblob_subjectivity\n",
       "25000  story of a man who has unnatural feelings for ...  ...              0.620370\n",
       "25001  airport  starts as a brand new luxury  plane i...  ...              0.499230\n",
       "25002  this film lacked something i couldnt put my fi...  ...              0.527778\n",
       "25003  sorry everyone i know this is supposed to be a...  ...              0.647083\n",
       "25004  when i was little my parents took me along to ...  ...              0.557328\n",
       "...                                                  ...  ...                   ...\n",
       "49995  seeing as the vote average was pretty low and ...  ...              0.563529\n",
       "49996  the plot had some wretched unbelievable twists...  ...              0.600000\n",
       "49997  i am amazed at how this movieand most others h...  ...              0.645995\n",
       "49998  a christmas together actually came before my t...  ...              0.461614\n",
       "49999  workingclass romantic drama from director mart...  ...              0.635819\n",
       "\n",
       "[25000 rows x 5 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_df['textblob_polarity'] = imdb_df.apply(lambda x: TextBlob(x['review']).sentiment.polarity, axis=1)\n",
    "\n",
    "i = 0\n",
    "predicted_value = [ ] \n",
    "while i < len(imdb_df):\n",
    "    if ((imdb_df.iloc[i]['textblob_polarity'] >= 0)):\n",
    "        predicted_value.append('pos')\n",
    "        i = i+1\n",
    "    elif ((imdb_df.iloc[i]['textblob_polarity'] < 0)):\n",
    "        predicted_value.append('neg')\n",
    "        i = i+1        \n",
    "imdb_df['textblob_predicted'] = predicted_value\n",
    "\n",
    "imdb_df['textblob_subjectivity'] = imdb_df.apply(lambda x: TextBlob(x['review']).sentiment.subjectivity, axis=1)\n",
    "imdb_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "DWNs4Wq14gRt",
    "outputId": "c82e3b09-dbb9-4d15-93d6-79bbc88a1585"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score      : 0.7515891177218408\n",
      "Accuracy score: 0.68736\n"
     ]
    }
   ],
   "source": [
    "m1 = f1_score(imdb_df['label'], imdb_df['textblob_predicted'], pos_label='pos')\n",
    "m2 = accuracy_score(imdb_df['label'], imdb_df['textblob_predicted'])\n",
    "print('F1 score      :',m1)\n",
    "print('Accuracy score:',m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pEeI5plRe9yv"
   },
   "source": [
    "The performance of TextBlob in movie review dataset is slightly better than Twitter dataset as the F1 score and accuracy score are 0.752 and 0.687 respectively, which one of the measure exceeds 0.7 . Besides, we can notice that most of the reviews produce subjectivity score that exceeds 0.5, indicating they are fairly subjective statement.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Amazon's Products review dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 808
    },
    "colab_type": "code",
    "id": "3EsPK9n3bpk2",
    "outputId": "e146ac29-b165-480e-bd70-1a6ba852ce1d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>ratings</th>\n",
       "      <th>textblob_polarity</th>\n",
       "      <th>textblob_predicted</th>\n",
       "      <th>textblob_subjectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>a crappy cardboard ghost of the original  hard...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.305556</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.763889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>we have this same game but it was made in  we ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.081389</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.415000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>hated this productpredictable  not fun  it att...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.194792</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.463542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>i had high hopes for this game as i am a big f...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.024593</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.419584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>thought this was a book with pages to illustra...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.189231</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.344808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6884</th>\n",
       "      <td>my    yo love this puzzle  there is enough of ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.210000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.566667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6887</th>\n",
       "      <td>my  year old got this last year and still love...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.204167</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.412500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6888</th>\n",
       "      <td>this puzzle is very well made  the pieces are ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.177546</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.531812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6889</th>\n",
       "      <td>we love the melissa and doug line we have abou...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.219123</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.506364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6890</th>\n",
       "      <td>we bought this pirate ship puzzle along witht ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.229091</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.436818</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22500 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                reviews  ...  textblob_subjectivity\n",
       "157   a crappy cardboard ghost of the original  hard...  ...               0.763889\n",
       "165   we have this same game but it was made in  we ...  ...               0.415000\n",
       "186   hated this productpredictable  not fun  it att...  ...               0.463542\n",
       "191   i had high hopes for this game as i am a big f...  ...               0.419584\n",
       "298   thought this was a book with pages to illustra...  ...               0.344808\n",
       "...                                                 ...  ...                    ...\n",
       "6884  my    yo love this puzzle  there is enough of ...  ...               0.566667\n",
       "6887  my  year old got this last year and still love...  ...               0.412500\n",
       "6888  this puzzle is very well made  the pieces are ...  ...               0.531812\n",
       "6889  we love the melissa and doug line we have abou...  ...               0.506364\n",
       "6890  we bought this pirate ship puzzle along witht ...  ...               0.436818\n",
       "\n",
       "[22500 rows x 5 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amz_df['textblob_polarity'] = amz_df.apply(lambda x: TextBlob(x['reviews']).sentiment.polarity, axis=1)\n",
    "\n",
    "i = 0\n",
    "predicted_value = [ ] \n",
    "while i < len(amz_df):\n",
    "    if ((amz_df.iloc[i]['textblob_polarity'] > 0.6)):\n",
    "        predicted_value.append(5.0)\n",
    "        i = i+1\n",
    "    elif ((amz_df.iloc[i]['textblob_polarity'] <= 0.6 and amz_df.iloc[i]['textblob_polarity'] > 0.2)):\n",
    "        predicted_value.append(4.0)\n",
    "        i = i+1 \n",
    "    elif ((amz_df.iloc[i]['textblob_polarity'] <= 0.2 and amz_df.iloc[i]['textblob_polarity'] > -0.2)):\n",
    "        predicted_value.append(3.0)\n",
    "        i = i+1\n",
    "    elif ((amz_df.iloc[i]['textblob_polarity'] <= -0.2 and amz_df.iloc[i]['textblob_polarity']> -0.6)):\n",
    "        predicted_value.append(2.0)\n",
    "        i = i+1\n",
    "    elif ((amz_df.iloc[i]['textblob_polarity'] <= -0.6)):\n",
    "        predicted_value.append(1.0)\n",
    "        i = i+1         \n",
    "amz_df['textblob_predicted'] = predicted_value\n",
    "\n",
    "amz_df['textblob_subjectivity'] = amz_df.apply(lambda x: TextBlob(x['reviews']).sentiment.subjectivity, axis=1)\n",
    "amz_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "GPs9rKbFUHx4",
    "outputId": "7185f0eb-a1fd-4968-b4ac-1f5bed955d72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score      : 0.15556798102787572\n",
      "Accuracy score: 0.22924444444444445\n"
     ]
    }
   ],
   "source": [
    "m1 = f1_score(amz_df['ratings'], amz_df['textblob_predicted'], average='macro')\n",
    "m2 = accuracy_score(amz_df['ratings'], amz_df['textblob_predicted'])\n",
    "print('F1 score      :',m1)\n",
    "print('Accuracy score:',m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1bFaFZubzDfF"
   },
   "source": [
    "In conclusion, TextBlob shines if your intention is having a quick prediction on the sentiment of documents but be awared that the accuracy is lower compared to custom trained model like Logistic Regression. \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# VADER \n",
    "\n",
    "Widely used in analyzing sentiment on social media text because it has been specifically attuned to analyze sentiments expressed in social media (Eg: Twitter, Facebook etc)\n",
    "\n",
    "VADER produces four sentiment metrics:\n",
    " 1. positive\n",
    " 2. neutral \n",
    " 3. negative \n",
    " 4. compound\n",
    " \n",
    "The first three, positive, neutral and negative, represent the proportion of the text that falls into those categories. \n",
    "\n",
    "The compound score is a normalized score of **sum_s** and\n",
    "**sum_s** is the sum of the valence score computed based on pre-defined sentiment lexicon (aka Sentiment Intensity) and\n",
    "the normalized score is simply the **sum_s** divided by square root of  its square plus an alpha parameter (a hyperparameter).\n",
    "\n",
    "> norm(sum_s) = sum_s / sqrt(sum_s*sum_s + alpha)\n",
    " \n",
    "\n",
    "## Pros\n",
    "1. Among the most comprehensive tools for social media analysis \n",
    "\n",
    "2. Easy to use and does not require training process\n",
    "\n",
    "##  Cons\n",
    "1. Not accurate when dealing when longer text and complex data\n",
    "\n",
    "2. Does not recognize context of the datasets, hence the result generated are less accurate when dealing with datasets \n",
    "\n",
    "Reference: \n",
    "1. https://www.kaggle.com/nikhilsable/sentiment-using-airline-tweets-using-vader\n",
    "\n",
    "2. http://t-redactyl.io/blog/2017/04/using-vader-to-handle-sentiment-analysis-with-social-media-text.html\n",
    "\n",
    "### 1. Twitter dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 942
    },
    "colab_type": "code",
    "id": "AOuacolEzFk1",
    "outputId": "201496b3-d2f8-42e0-b163-8f2d52b1f13d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting vaderSentiment\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/86/9e/c53e1fc61aac5ee490a6ac5e21b1ac04e55a7c2aba647bb8411c9aadf24e/vaderSentiment-3.2.1-py2.py3-none-any.whl (125kB)\n",
      "\r",
      "\u001b[K     |██▋                             | 10kB 20.0MB/s eta 0:00:01\r",
      "\u001b[K     |█████▏                          | 20kB 1.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████▉                        | 30kB 2.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▍                     | 40kB 1.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████                   | 51kB 2.1MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▋                | 61kB 2.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▎             | 71kB 2.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▉           | 81kB 3.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▍        | 92kB 3.6MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████      | 102kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▋   | 112kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▎| 122kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 133kB 2.8MB/s \n",
      "\u001b[?25hInstalling collected packages: vaderSentiment\n",
      "Successfully installed vaderSentiment-3.2.1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>polarity</th>\n",
       "      <th>textblob_polarity</th>\n",
       "      <th>textblob_predicted</th>\n",
       "      <th>textblob_subjectivity</th>\n",
       "      <th>Vader_score</th>\n",
       "      <th>Vader_predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>switchfoot httptwitpiccomyzl  awww thats a bum...</td>\n",
       "      <td>neg</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.4500</td>\n",
       "      <td>-0.3818</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is upset that he cant update his facebook by t...</td>\n",
       "      <td>neg</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.7269</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kenichan i dived many times for the ball manag...</td>\n",
       "      <td>neg</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.4939</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>neg</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>-0.2500</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nationwideclass no its not behaving at all im ...</td>\n",
       "      <td>neg</td>\n",
       "      <td>-0.6250</td>\n",
       "      <td>neg</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>-0.6597</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>just woke up having no school is the best feel...</td>\n",
       "      <td>pos</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>0.5423</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>thewdbcom  very cool to hear old walt intervie...</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.2775</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.5225</td>\n",
       "      <td>0.3804</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>are you ready for your mojo makeover ask me fo...</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.3612</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>happy th birthday to my boo of alll time tupac...</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.8000</td>\n",
       "      <td>pos</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.5719</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>happy charitytuesday thenspcc sparkscharity sp...</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.8000</td>\n",
       "      <td>pos</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.5719</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text  ... Vader_predicted\n",
       "0        switchfoot httptwitpiccomyzl  awww thats a bum...  ...             neg\n",
       "1        is upset that he cant update his facebook by t...  ...             neg\n",
       "2        kenichan i dived many times for the ball manag...  ...             pos\n",
       "3          my whole body feels itchy and like its on fire   ...             neg\n",
       "4        nationwideclass no its not behaving at all im ...  ...             neg\n",
       "...                                                    ...  ...             ...\n",
       "1599995  just woke up having no school is the best feel...  ...             pos\n",
       "1599996  thewdbcom  very cool to hear old walt intervie...  ...             pos\n",
       "1599997  are you ready for your mojo makeover ask me fo...  ...             pos\n",
       "1599998  happy th birthday to my boo of alll time tupac...  ...             pos\n",
       "1599999  happy charitytuesday thenspcc sparkscharity sp...  ...             pos\n",
       "\n",
       "[200000 rows x 7 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install vaderSentiment\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "i = 0\n",
    "comp_value = []\n",
    "while i < len(tweet_df):\n",
    "    k = analyser.polarity_scores(tweet_df.iloc[i]['text'])\n",
    "    comp_value.append(k['compound'])\n",
    "    i += 1\n",
    "    \n",
    "comp_value = np.array(comp_value)\n",
    "tweet_df['Vader_score'] = comp_value\n",
    "\n",
    "i = 0\n",
    "predicted_value = [ ] \n",
    "while i < len(tweet_df):\n",
    "    if ((tweet_df.iloc[i]['Vader_score'] >= 0)):\n",
    "        predicted_value.append('pos')\n",
    "        i = i+1\n",
    "    elif ((tweet_df.iloc[i]['Vader_score'] < 0)):\n",
    "        predicted_value.append('neg')\n",
    "        i = i+1\n",
    "        \n",
    "tweet_df['Vader_predicted'] = predicted_value\n",
    "tweet_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "5vhLkWoq4yzJ",
    "outputId": "929e9a8a-e490-4173-89d5-610cd5ba9deb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score      : 0.7211264630247287\n",
      "Accuracy score: 0.656775\n"
     ]
    }
   ],
   "source": [
    "m1 = f1_score(tweet_df['polarity'], tweet_df['Vader_predicted'], pos_label='pos')\n",
    "m2 = accuracy_score(tweet_df['polarity'], tweet_df['Vader_predicted'])\n",
    "print('F1 score      :',m1)\n",
    "print('Accuracy score:',m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ry4BfHLveFgb"
   },
   "source": [
    "VADER is doing well in predicting the sentiment for Twitter dataset as the F1 score and accuracy score are 0.721 and 0.657 respectively, which are better than the result produced by TextBlob. This proves that VADER is better in analysing social media text than TextBlob.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. IMDB movie review dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "STfGOP4kLljB",
    "outputId": "0a1185ba-9963-43d1-ce6e-b9d7f081fb2d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "      <th>textblob_polarity</th>\n",
       "      <th>textblob_predicted</th>\n",
       "      <th>textblob_subjectivity</th>\n",
       "      <th>Vader_score</th>\n",
       "      <th>Vader_predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25000</th>\n",
       "      <td>story of a man who has unnatural feelings for ...</td>\n",
       "      <td>neg</td>\n",
       "      <td>-0.071759</td>\n",
       "      <td>neg</td>\n",
       "      <td>0.620370</td>\n",
       "      <td>0.7003</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25001</th>\n",
       "      <td>airport  starts as a brand new luxury  plane i...</td>\n",
       "      <td>neg</td>\n",
       "      <td>0.040492</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.499230</td>\n",
       "      <td>-0.9657</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25002</th>\n",
       "      <td>this film lacked something i couldnt put my fi...</td>\n",
       "      <td>neg</td>\n",
       "      <td>0.079167</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.527778</td>\n",
       "      <td>0.8936</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25003</th>\n",
       "      <td>sorry everyone i know this is supposed to be a...</td>\n",
       "      <td>neg</td>\n",
       "      <td>0.043542</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.647083</td>\n",
       "      <td>0.8684</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25004</th>\n",
       "      <td>when i was little my parents took me along to ...</td>\n",
       "      <td>neg</td>\n",
       "      <td>-0.055741</td>\n",
       "      <td>neg</td>\n",
       "      <td>0.557328</td>\n",
       "      <td>-0.9757</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>seeing as the vote average was pretty low and ...</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.291961</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.563529</td>\n",
       "      <td>0.9925</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>the plot had some wretched unbelievable twists...</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.8934</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>i am amazed at how this movieand most others h...</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.136099</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.645995</td>\n",
       "      <td>0.9594</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>a christmas together actually came before my t...</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.118069</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.461614</td>\n",
       "      <td>0.9865</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>workingclass romantic drama from director mart...</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.121951</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.635819</td>\n",
       "      <td>0.8937</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  ... Vader_predicted\n",
       "25000  story of a man who has unnatural feelings for ...  ...             pos\n",
       "25001  airport  starts as a brand new luxury  plane i...  ...             neg\n",
       "25002  this film lacked something i couldnt put my fi...  ...             pos\n",
       "25003  sorry everyone i know this is supposed to be a...  ...             pos\n",
       "25004  when i was little my parents took me along to ...  ...             neg\n",
       "...                                                  ...  ...             ...\n",
       "49995  seeing as the vote average was pretty low and ...  ...             pos\n",
       "49996  the plot had some wretched unbelievable twists...  ...             pos\n",
       "49997  i am amazed at how this movieand most others h...  ...             pos\n",
       "49998  a christmas together actually came before my t...  ...             pos\n",
       "49999  workingclass romantic drama from director mart...  ...             pos\n",
       "\n",
       "[25000 rows x 7 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyser = SentimentIntensityAnalyzer()\n",
    "i = 0\n",
    "comp_value = []\n",
    "while i < len(imdb_df):\n",
    "    k = analyser.polarity_scores(imdb_df.iloc[i]['review'])\n",
    "    comp_value.append(k['compound'])\n",
    "    i += 1\n",
    "    \n",
    "comp_value = np.array(comp_value)\n",
    "imdb_df['Vader_score'] = comp_value\n",
    "\n",
    "i = 0\n",
    "predicted_value = [ ] \n",
    "while i < len(imdb_df):\n",
    "    if ((imdb_df.iloc[i]['Vader_score'] >= 0)):\n",
    "        predicted_value.append('pos')\n",
    "        i = i+1\n",
    "    elif ((imdb_df.iloc[i]['Vader_score'] < 0)):\n",
    "        predicted_value.append('neg')\n",
    "        i = i+1\n",
    "        \n",
    "imdb_df['Vader_predicted'] = predicted_value\n",
    "imdb_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "CRDJHfRw5AQ5",
    "outputId": "dc3d5ddf-f11d-46b2-877d-dae2f0527c06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score      : 0.7342437337942956\n",
      "Accuracy score: 0.69252\n"
     ]
    }
   ],
   "source": [
    "m1 = f1_score(imdb_df['label'], imdb_df['Vader_predicted'], pos_label='pos')\n",
    "m2 = accuracy_score(imdb_df['label'], imdb_df['Vader_predicted'])\n",
    "print('F1 score      :',m1)\n",
    "print('Accuracy score:',m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mujoRrVVfAH4"
   },
   "source": [
    "Different scenario happens here as the result generated by VADER for this dataset (F1 score: 0.734 and accuracy score: 0.693) are less accurate compared to TextBlob, this maybe due to VADER is weak in handling longer and more complex text.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Amazon's Products review dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 808
    },
    "colab_type": "code",
    "id": "m0Pzsy3yb2lQ",
    "outputId": "29c67c5f-fba1-42f4-ccea-1e91b2ae9816"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>ratings</th>\n",
       "      <th>textblob_polarity</th>\n",
       "      <th>textblob_predicted</th>\n",
       "      <th>textblob_subjectivity</th>\n",
       "      <th>Vader_score</th>\n",
       "      <th>Vader_predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>a crappy cardboard ghost of the original  hard...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.305556</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.763889</td>\n",
       "      <td>-0.9052</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>we have this same game but it was made in  we ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.081389</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.415000</td>\n",
       "      <td>0.7964</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>hated this productpredictable  not fun  it att...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.194792</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.463542</td>\n",
       "      <td>-0.1205</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>i had high hopes for this game as i am a big f...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.024593</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.419584</td>\n",
       "      <td>0.9607</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>thought this was a book with pages to illustra...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.189231</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.344808</td>\n",
       "      <td>-0.9230</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6884</th>\n",
       "      <td>my    yo love this puzzle  there is enough of ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.210000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.4118</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6887</th>\n",
       "      <td>my  year old got this last year and still love...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.204167</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.412500</td>\n",
       "      <td>0.8689</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6888</th>\n",
       "      <td>this puzzle is very well made  the pieces are ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.177546</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.531812</td>\n",
       "      <td>0.9894</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6889</th>\n",
       "      <td>we love the melissa and doug line we have abou...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.219123</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.506364</td>\n",
       "      <td>0.8573</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6890</th>\n",
       "      <td>we bought this pirate ship puzzle along witht ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.229091</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.436818</td>\n",
       "      <td>0.7172</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22500 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                reviews  ...  Vader_predicted\n",
       "157   a crappy cardboard ghost of the original  hard...  ...              1.0\n",
       "165   we have this same game but it was made in  we ...  ...              5.0\n",
       "186   hated this productpredictable  not fun  it att...  ...              3.0\n",
       "191   i had high hopes for this game as i am a big f...  ...              5.0\n",
       "298   thought this was a book with pages to illustra...  ...              1.0\n",
       "...                                                 ...  ...              ...\n",
       "6884  my    yo love this puzzle  there is enough of ...  ...              4.0\n",
       "6887  my  year old got this last year and still love...  ...              5.0\n",
       "6888  this puzzle is very well made  the pieces are ...  ...              5.0\n",
       "6889  we love the melissa and doug line we have abou...  ...              5.0\n",
       "6890  we bought this pirate ship puzzle along witht ...  ...              5.0\n",
       "\n",
       "[22500 rows x 7 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyser = SentimentIntensityAnalyzer()\n",
    "i = 0\n",
    "comp_value = []\n",
    "while i < len(amz_df):\n",
    "    k = analyser.polarity_scores(amz_df.iloc[i]['reviews'])\n",
    "    comp_value.append(k['compound'])\n",
    "    i += 1\n",
    "    \n",
    "comp_value = np.array(comp_value)\n",
    "amz_df['Vader_score'] = comp_value\n",
    "\n",
    "i = 0\n",
    "predicted_value = [ ] \n",
    "while i < len(amz_df):\n",
    "    if ((amz_df.iloc[i]['Vader_score'] > 0.6)):\n",
    "        predicted_value.append(5.0)\n",
    "        i = i+1\n",
    "    elif ((amz_df.iloc[i]['Vader_score'] <= 0.6 and amz_df.iloc[i]['Vader_score'] > 0.2)):\n",
    "        predicted_value.append(4.0)\n",
    "        i = i+1 \n",
    "    elif ((amz_df.iloc[i]['Vader_score'] <= 0.2 and amz_df.iloc[i]['Vader_score'] > -0.2)):\n",
    "        predicted_value.append(3.0)\n",
    "        i = i+1\n",
    "    elif ((amz_df.iloc[i]['Vader_score'] <= -0.2 and amz_df.iloc[i]['Vader_score']> -0.6)):\n",
    "        predicted_value.append(2.0)\n",
    "        i = i+1\n",
    "    elif ((amz_df.iloc[i]['Vader_score'] <= -0.6)):\n",
    "        predicted_value.append(1.0)\n",
    "        i = i+1         \n",
    "amz_df['Vader_predicted'] = predicted_value\n",
    "amz_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "3VtRQdFHUn3z",
    "outputId": "2e64cb98-eadb-4a2b-9c7f-503c47145e56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score      : 0.23950762351467847\n",
      "Accuracy score: 0.2876444444444444\n"
     ]
    }
   ],
   "source": [
    "m1 = f1_score(amz_df['ratings'], amz_df['Vader_predicted'], average='macro')\n",
    "m2 = accuracy_score(amz_df['ratings'], amz_df['Vader_predicted'])\n",
    "print('F1 score      :',m1)\n",
    "print('Accuracy score:',m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a1iBt9ap1GDW"
   },
   "source": [
    "Again, the result produced by VADER for this dataset is almost the same situation as the second dataset, where it's less accurate compared to using TextBlob. In short, VADER really shines in analysing social media text like the Twitter dataset. However, VADER don't do well in handling longer text and complex sentence like in the movie review dataset.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# Logistic Regression\n",
    "\n",
    "## Why not Linear Regression?\n",
    "    \n",
    "Linear regression is unbounded as the predicted value (output) can exceed 0 and 1 range. This brings logistic regression into picture as their ouput value strictly ranges from 0 to 1, which is suitable for classification problem.\n",
    "\n",
    "![alt text](https://miro.medium.com/max/2900/1*dm6ZaX5fuSmuVvM4Ds-vcg.jpeg)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## What is Logistic Regression?\n",
    "\n",
    "Logistic regression is a classification algorithm used to assign observations (input) to a discrete set of classes (often in binary). Some of the examples of classification problems are Email spam or not spam, Online transactions Fraud or not Fraud, Tumor Malignant or Benign. \n",
    "\n",
    "Logistic regression transforms its output using the logistic sigmoid function to return a probability value. Sigmoid function maps any real value into another value between 0 and 1, just like probability. \n",
    "![alt text](https://miro.medium.com/max/800/1*OUOB_YF41M-O4GgZH_F2rw.png)\n",
    "\n",
    "## Formula of sigmoid function\n",
    "\n",
    "![alt text](https://miro.medium.com/max/339/1*Gp5E23P5d2PY5D5kOo8ePw.png)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## How sigmoid function is derived though?\n",
    "\n",
    "It is derived from the log(odds) function, log [p/(1-p)].\n",
    "\n",
    "Odds is defined as probability of success/probability of failure. So the odds of a success (80% chance of rain) has an accompanying odds of failure (20% chance it doesn’t rain); as an equation (the “odds ratio“), that’s .8/.2 = 4. Conversion to log odds results in symmetry around zero, which is easier for analysis.\n",
    "\n",
    "The name 'logistic regression' can be justified as data is fit into linear regression model, which then applied by a logistic (sigmoid) function to produce binary outcome and predict the target categorical dependent variable.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Pros\n",
    "1. doesn't require high computation power\n",
    "\n",
    "2. easy to implement, interpret and very efficient to train. \n",
    "\n",
    "## Cons\n",
    "1. Main limitation of Logistic Regression is the assumption of linearity between the dependent variable and the independent variables. In the real world,the data is rarely linearly separable. Most of the time data would be a jumbled mess.\n",
    "\n",
    "2. If the number of features are less than the number of classes, it \n",
    "tends to overfit.\n",
    "\n",
    "3. will not perform well with independent variables that are not correlated to the target variable\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Cross validation\n",
    "\n",
    "One way to prevent overfitting is to perform cross validation. The idea is to\n",
    "\n",
    "1. partition your data into training data and testing data (sometimes called validation data); treat testing data as unobserved, and\n",
    "\n",
    "2. fit your model using only the training data.\n",
    "\n",
    "3. evaluate your model on the testing data that you held out earlier, compare with the actual results, and obtain a testing error.\n",
    "\n",
    "Repeat the process K times then take average of the testing errors as a final performance measure.\n",
    "\n",
    "Reference:\n",
    "1. https://towardsdatascience.com/sentiment-analysis-with-python-part-2-4f71e7bde59a\n",
    "\n",
    "2. https://towardsdatascience.com/introduction-to-logistic-regression-66248243c148\n",
    "\n",
    "3. https://machinelearningmastery.com/logistic-regression-for-machine-learning/\n",
    "\n",
    "### 1. Twitter dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "yhf2RZC_5UWE",
    "outputId": "494903f8-db4f-4151-9d6d-2c90d7494565"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.01: 0.750875\n",
      "Accuracy for C=0.05: 0.770025\n",
      "Accuracy for C=0.25: 0.77995\n",
      "Accuracy for C=0.5: 0.78075\n",
      "Accuracy for C=1: 0.780975\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#using n-grams\n",
    "ngram_vectorizer = CountVectorizer(ngram_range=(1, 2), \n",
    "                                   stop_words=\"english\", \n",
    "                                   binary=True)\n",
    "ngram_vectorizer.fit(tweet_df[\"text\"])\n",
    "X = ngram_vectorizer.transform(tweet_df[\"text\"])\n",
    "target = tweet_df[\"polarity\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, target, train_size = 0.8)\n",
    "\n",
    "for c in [0.01, 0.05, 0.25, 0.5, 1]:    \n",
    "    lr = LogisticRegression(C=c, max_iter=1000)\n",
    "    lr.fit(X_train, y_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(y_test, lr.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "rv0ZImX9PZxO",
    "outputId": "b93f240a-c1b9-4fd2-9d41-b2b7aa8ef755"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.01: 0.757575\n",
      "Accuracy for C=0.05: 0.7716\n",
      "Accuracy for C=0.25: 0.780525\n",
      "Accuracy for C=0.5: 0.782025\n",
      "Accuracy for C=1: 0.78225\n"
     ]
    }
   ],
   "source": [
    "#using count vectorizer\n",
    "wc_vectorizer = CountVectorizer(ngram_range=(1,3),\n",
    "                                stop_words=\"english\")\n",
    "wc_vectorizer.fit(tweet_df[\"text\"])\n",
    "X = wc_vectorizer.transform(tweet_df[\"text\"])\n",
    "target = tweet_df[\"polarity\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, target, train_size = 0.8)\n",
    "\n",
    "for c in [0.01, 0.05, 0.25, 0.5, 1]:    \n",
    "    lr = LogisticRegression(C=c, max_iter=1000)\n",
    "    lr.fit(X_train, y_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(y_test, lr.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "8Xlkg3z_Q5Cw",
    "outputId": "c9cdd1a2-f146-4628-a31f-f64b01d8c1f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.01: 0.7248\n",
      "Accuracy for C=0.05: 0.740175\n",
      "Accuracy for C=0.25: 0.7606\n",
      "Accuracy for C=0.5: 0.768975\n",
      "Accuracy for C=1: 0.7742\n"
     ]
    }
   ],
   "source": [
    "#using tfidf vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,3),\n",
    "                                   stop_words='english')\n",
    "tfidf_vectorizer.fit(tweet_df[\"text\"])\n",
    "X = tfidf_vectorizer.transform(tweet_df[\"text\"])\n",
    "target = tweet_df[\"polarity\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, target, train_size = 0.8)\n",
    "\n",
    "for c in [0.01, 0.05, 0.25, 0.5, 1]:    \n",
    "    lr = LogisticRegression(C=c, max_iter=1000)\n",
    "    lr.fit(X_train, y_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(y_test, lr.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZsIsEjYLeJsd"
   },
   "source": [
    "For Twitter dataset, the best accuracy score falls around the value of 0.78 for different pre-processing methods used and count vectorizer works better in this case. Another thing to take note is that the accuracy score is higher compared to using TextBlob and VADER.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. IMDB movie review dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "oh_OW-wIPEfA",
    "outputId": "58fdbfb6-27fc-4974-8993-7aad6ef0d6b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.01: 0.871\n",
      "Accuracy for C=0.05: 0.8758\n",
      "Accuracy for C=0.25: 0.8788\n",
      "Accuracy for C=0.5: 0.879\n",
      "Accuracy for C=1: 0.8796\n"
     ]
    }
   ],
   "source": [
    "#using n-grams only\n",
    "ngram_vectorizer = CountVectorizer(ngram_range=(1, 3), \n",
    "                                   stop_words=\"english\", \n",
    "                                   binary=True)\n",
    "ngram_vectorizer.fit(imdb_df[\"review\"])\n",
    "X = ngram_vectorizer.transform(imdb_df[\"review\"])\n",
    "target = imdb_df[\"label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, target, train_size = 0.8)\n",
    "\n",
    "for c in [0.01, 0.05, 0.25, 0.5, 1]:    \n",
    "    lr = LogisticRegression(C=c, max_iter=1000)\n",
    "    lr.fit(X_train, y_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(y_test, lr.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "alk1WbXy02Gy",
    "outputId": "3ea99e10-e06c-4918-e50e-0c04a79d1874"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.01: 0.8806\n",
      "Accuracy for C=0.05: 0.8856\n",
      "Accuracy for C=0.25: 0.8886\n",
      "Accuracy for C=0.5: 0.8878\n",
      "Accuracy for C=1: 0.8878\n"
     ]
    }
   ],
   "source": [
    "#using count vectorizer\n",
    "wc_vectorizer = CountVectorizer(ngram_range=(1, 3),\n",
    "                                stop_words=\"english\")\n",
    "wc_vectorizer.fit(imdb_df[\"review\"])\n",
    "X = wc_vectorizer.transform(imdb_df[\"review\"])\n",
    "target = imdb_df[\"label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, target, train_size = 0.8, )\n",
    "\n",
    "for c in [0.01, 0.05, 0.25, 0.5, 1]:    \n",
    "    lr = LogisticRegression(C=c, max_iter=1000)\n",
    "    lr.fit(X_train, y_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(y_test, lr.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "-XqOc6ZvQcz9",
    "outputId": "7a2f6e40-84c8-4829-af4b-4aab218bb2c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.01: 0.79\n",
      "Accuracy for C=0.05: 0.8116\n",
      "Accuracy for C=0.25: 0.8396\n",
      "Accuracy for C=0.5: 0.855\n",
      "Accuracy for C=1: 0.8664\n"
     ]
    }
   ],
   "source": [
    "#using tfidf vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 3),\n",
    "                                   stop_words='english')\n",
    "tfidf_vectorizer.fit(imdb_df[\"review\"])\n",
    "X = tfidf_vectorizer.transform(imdb_df[\"review\"])\n",
    "target = imdb_df[\"label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, target, train_size = 0.8, )\n",
    "\n",
    "for c in [0.01, 0.05, 0.25, 0.5, 1]:    \n",
    "    lr = LogisticRegression(C=c, max_iter=1000)\n",
    "    lr.fit(X_train, y_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(y_test, lr.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8BNmhN-tfDjd"
   },
   "source": [
    "Logistic regression works quite well for movie review dataset as the accuracy score is within the range of 0.87 and 0.89 for different pre-processing methods used and again, count vectorizer works best in this case.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Amazon's Products review dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "o0XNL3VDcK5t",
    "outputId": "2dc6dfcd-1c83-4b87-e9c2-698a6ab15b15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.01: 0.4915555555555556\n",
      "Accuracy for C=0.05: 0.5068888888888889\n",
      "Accuracy for C=0.25: 0.5008888888888889\n",
      "Accuracy for C=0.5: 0.49822222222222223\n",
      "Accuracy for C=1: 0.49644444444444447\n"
     ]
    }
   ],
   "source": [
    "#using n-grams\n",
    "ngram_vectorizer = CountVectorizer(ngram_range=(1, 3), \n",
    "                                   stop_words=\"english\", \n",
    "                                   binary=True)\n",
    "ngram_vectorizer.fit(amz_df[\"reviews\"])\n",
    "X = ngram_vectorizer.transform(amz_df[\"reviews\"])\n",
    "target = amz_df[\"ratings\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, target, train_size = 0.8)\n",
    "\n",
    "for c in [0.01, 0.05, 0.25, 0.5, 1]:    \n",
    "    lr = LogisticRegression(C=c, max_iter=1000, multi_class='multinomial')\n",
    "    lr.fit(X_train, y_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(y_test, lr.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "SHMOW59gclsy",
    "outputId": "cd29af54-9c9e-49a2-c75b-32b17c44c827"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.01: 0.4948888888888889\n",
      "Accuracy for C=0.05: 0.5044444444444445\n",
      "Accuracy for C=0.25: 0.5035555555555555\n",
      "Accuracy for C=0.5: 0.5024444444444445\n",
      "Accuracy for C=1: 0.49977777777777777\n"
     ]
    }
   ],
   "source": [
    "#using count vectorizer\n",
    "wc_vectorizer = CountVectorizer(ngram_range=(1, 3),\n",
    "                                stop_words=\"english\")\n",
    "wc_vectorizer.fit(amz_df[\"reviews\"])\n",
    "X = wc_vectorizer.transform(amz_df[\"reviews\"])\n",
    "target = amz_df[\"ratings\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, target, train_size = 0.8)\n",
    "\n",
    "for c in [0.01, 0.05, 0.25, 0.5, 1]:    \n",
    "    lr = LogisticRegression(C=c, max_iter=1000)\n",
    "    lr.fit(X_train, y_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(y_test, lr.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "NgcQPLCKc-KI",
    "outputId": "a01f1f04-8968-489b-cafa-8740cb134489"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.01: 0.4448888888888889\n",
      "Accuracy for C=0.05: 0.46111111111111114\n",
      "Accuracy for C=0.25: 0.484\n",
      "Accuracy for C=0.5: 0.494\n",
      "Accuracy for C=1: 0.504\n"
     ]
    }
   ],
   "source": [
    "#using tf-idf vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 3),\n",
    "                                   stop_words='english')\n",
    "tfidf_vectorizer.fit(amz_df[\"reviews\"])\n",
    "X = tfidf_vectorizer.transform(amz_df[\"reviews\"])\n",
    "target = amz_df[\"ratings\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, target, train_size = 0.8)\n",
    "\n",
    "for c in [0.01, 0.05, 0.25, 0.5, 1]:    \n",
    "    lr = LogisticRegression(C=c, max_iter=1000)\n",
    "    lr.fit(X_train, y_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(y_test, lr.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hLIfWDa21QF7"
   },
   "source": [
    "Logistic regression don't do well in predicting the sentiment of this dataset with just getting accuracy score of around 0.50. However, it's doing significantly better than TextBlob and Vader (0.23 and 0.29 respectively)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# Naive Bayes\n",
    "It is a classification technique based on Bayes’ Theorem with an assumption of independence among predictors. This theorem provides a way of calculating a type or probability called posterior probability, in which the probability of a event A (like the review is positive/negative) occurring is reliant on probabilistic known background (e.g. event B evidence).\n",
    "\n",
    "![alt text](https://miro.medium.com/max/918/1*LB-G6WBuswEfpg20FMighA.png)\n",
    "\n",
    "## But why is it called ‘Naive’?\n",
    "\n",
    "Naive Bayes classifier assumes that all the features are unrelated to each other. Presence or absence of a feature does not influence the presence or absence of any other feature. In real datasets, we test a hypothesis given multiple evidence(feature). So, calculations become complicated. To simplify the work, the feature independence approach is used to ‘uncouple’ multiple evidence and treat each as an independent one.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## How Naive Bayes works?\n",
    "\n",
    "![alt text](https://drive.google.com/uc?id=1QR2fjZtuVEyYhDllWTt3U-PS4qnc2CFd)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Pros\n",
    "1. easy and fast to predict class of test data set. It is also a highly scalable algorithm.\n",
    "\n",
    "2. can be used for Binary and Multiclass classification\n",
    "\n",
    "3. performs better compare to other models like logistic regression and you need less training data (p.s: if the assumption of independence holds)\n",
    "\n",
    "4. perform well in case of categorical input variables compared to numerical variable(s)\n",
    "\n",
    "## Cons\n",
    "1. If categorical variable has a category (in test data set), which was not observed in training data set, then model will assign a 0 (zero) probability and will be unable to make a prediction. This is often known as “Zero Frequency\" (p.s: can be solved by smoothing technique, such as Laplace smoothing, basically add 1 to every count so it’s never zero)\n",
    "\n",
    "2. Naive Bayes is based on the asssumption of the independence of predictors. In real life, it is almost impossible that we get a set of predictors which are completely independent.\n",
    "\n",
    "3. Naive Bayes can learn individual features importance but can’t determine the relationship among features.\n",
    "\n",
    "Reference: \n",
    "1. https://www.machinelearningplus.com/predictive-modeling/how-naive-bayes-algorithm-works-with-example-and-full-code/\n",
    "\n",
    "2. https://levelup.gitconnected.com/movie-review-sentiment-analysis-with-naive-bayes-machine-learning-from-scratch-part-v-7bb869391bab\n",
    "\n",
    "### 1. Twitter dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "ye9msFer1T4T",
    "outputId": "6f054760-62db-4db0-a5a2-7ef6ab8a6d53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score      : 0.7604726100966702\n",
      "Accuracy score: 0.771425\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import f1_score,accuracy_score\n",
    "\n",
    "wc_vectorizer = CountVectorizer(ngram_range=(1, 3),\n",
    "                                stop_words='english')\n",
    "wc_vectorizer.fit(tweet_df[\"text\"])\n",
    "X = wc_vectorizer.transform(tweet_df[\"text\"])\n",
    "target = tweet_df[\"polarity\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, target, train_size = 0.8)\n",
    "\n",
    "naive_bayes_classifier = MultinomialNB()\n",
    "naive_bayes_classifier.fit(X_train, y_train)\n",
    "y_pred = naive_bayes_classifier.predict(X_test)\n",
    "\n",
    "m1 = f1_score(y_test, y_pred, pos_label='pos')\n",
    "m2 = accuracy_score(y_test, y_pred)\n",
    "print('F1 score      :',m1)\n",
    "print('Accuracy score:',m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "w9aY4nqQtHMO",
    "outputId": "12f0f0fd-c7f7-451e-a549-4e2d0a6b7e23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score      : 0.7645965821360341\n",
      "Accuracy score: 0.775125\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 3), \n",
    "                                   stop_words='english')\n",
    "tfidf_vectorizer.fit(tweet_df[\"text\"])\n",
    "X = tfidf_vectorizer.transform(tweet_df[\"text\"])\n",
    "target = tweet_df[\"polarity\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, target, train_size = 0.8)\n",
    "\n",
    "naive_bayes_classifier = MultinomialNB()\n",
    "naive_bayes_classifier.fit(X_train, y_train)\n",
    "y_pred = naive_bayes_classifier.predict(X_test)\n",
    "\n",
    "m1 = f1_score(y_test, y_pred, pos_label='pos')\n",
    "m2 = accuracy_score(y_test, y_pred)\n",
    "print('F1 score      :',m1)\n",
    "print('Accuracy score:',m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "82cOeTLGU2lR"
   },
   "source": [
    "Naive Bayes is getting around the same accuracy with Logistic Regression with the accuracy score around 0.77 and TF-IDF works better in this case.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. IMDB movie review dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "R4jtdX6OZdMH",
    "outputId": "1e872a8b-09e1-446d-af7b-66e542b8e359"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score      : 0.8666114333057168\n",
      "Accuracy score: 0.8712\n"
     ]
    }
   ],
   "source": [
    "wc_vectorizer = CountVectorizer(ngram_range=(1, 3),\n",
    "                                stop_words='english')\n",
    "wc_vectorizer.fit(imdb_df[\"review\"])\n",
    "X = wc_vectorizer.transform(imdb_df[\"review\"])\n",
    "target = imdb_df[\"label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, target, train_size = 0.8)\n",
    "\n",
    "naive_bayes_classifier = MultinomialNB()\n",
    "naive_bayes_classifier.fit(X_train, y_train)\n",
    "y_pred = naive_bayes_classifier.predict(X_test)\n",
    "\n",
    "m1 = f1_score(y_test, y_pred, pos_label='pos')\n",
    "m2 = accuracy_score(y_test, y_pred)\n",
    "print('F1 score      :',m1)\n",
    "print('Accuracy score:',m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "XmGsJhSpaGW8",
    "outputId": "2e4789ca-74cd-436d-d04e-9cac9f9e3c44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score      : 0.878323932312651\n",
      "Accuracy score: 0.8792\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 3),\n",
    "                                  stop_words='english')\n",
    "tfidf_vectorizer.fit(imdb_df[\"review\"])\n",
    "X = tfidf_vectorizer.transform(imdb_df[\"review\"])\n",
    "target = imdb_df[\"label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, target, train_size = 0.8)\n",
    "\n",
    "naive_bayes_classifier = MultinomialNB()\n",
    "naive_bayes_classifier.fit(X_train, y_train)\n",
    "y_pred = naive_bayes_classifier.predict(X_test)\n",
    "\n",
    "m1 = f1_score(y_test, y_pred, pos_label='pos')\n",
    "m2 = accuracy_score(y_test, y_pred)\n",
    "print('F1 score      :',m1)\n",
    "print('Accuracy score:',m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dNjjGfAgfV9g"
   },
   "source": [
    "As the same as the first dataset, Naive Bayes achieces almost the same accuracy as Logistic Regression with accuracy score of 0.88 and TF-IDF works better in this case too.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### 3. Amazon's Products review dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "aF7Z0tIYapZD",
    "outputId": "3f63f5fc-3e64-47d0-d551-68aef419be05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score      : 0.46494102319457725\n",
      "Accuracy score: 0.46555555555555556\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "wc_vectorizer = CountVectorizer(ngram_range=(1, 3),\n",
    "                                stop_words='english')\n",
    "wc_vectorizer.fit(amz_df[\"reviews\"])\n",
    "X = wc_vectorizer.transform(amz_df[\"reviews\"])\n",
    "target = amz_df[\"ratings\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, target, train_size = 0.8)\n",
    "\n",
    "naive_bayes_classifier = MultinomialNB()\n",
    "naive_bayes_classifier.fit(X_train, y_train)\n",
    "y_pred = naive_bayes_classifier.predict(X_test)\n",
    "\n",
    "m1 = f1_score(y_test, y_pred, average='macro')\n",
    "m2 = accuracy_score(y_test, y_pred)\n",
    "print('F1 score      :',m1)\n",
    "print('Accuracy score:',m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "KDbAb3goa8mj",
    "outputId": "f95501f7-a2d5-4e9a-c346-e3ebd930a12a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score      : 0.49659489818015884\n",
      "Accuracy score: 0.506\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,3),\n",
    "                                   stop_words='english')\n",
    "tfidf_vectorizer.fit(amz_df[\"reviews\"])\n",
    "X = tfidf_vectorizer.transform(amz_df[\"reviews\"])\n",
    "target = amz_df[\"ratings\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, target, train_size = 0.8)\n",
    "\n",
    "naive_bayes_classifier = MultinomialNB()\n",
    "naive_bayes_classifier.fit(X_train, y_train)\n",
    "y_pred = naive_bayes_classifier.predict(X_test)\n",
    "\n",
    "m1 = f1_score(y_test, y_pred, average='macro')\n",
    "m2 = accuracy_score(y_test, y_pred)\n",
    "print('F1 score      :',m1)\n",
    "print('Accuracy score:',m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "32TJctR0VLVN"
   },
   "source": [
    "In conclusion, the accuracy of Naive Bayes are almost the same as Logistic Regression in three of the datasets. One thing to notice is that Naive Bayes tends to work better with TF-IDF than count vectorizer. \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# Support Vector Machine (SVM)\n",
    "\n",
    "## What is Support Vector Machine?\n",
    "\n",
    "The objective of the support vector machine algorithm is to find a hyperplane in an N-dimensional space (N — the number of features) that distinctly classifies the data points.\n",
    "\n",
    "![alt text](https://miro.medium.com/max/375/0*9jEWNXTAao7phK-5.png)\n",
    "![alt text](https://miro.medium.com/max/375/0*0o8xIA4k3gXUDCFU.png)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## What is hyperplane?\n",
    "\n",
    "To separate the two classes of data points, there are many possible hyperplanes that could be chosen. Hyperplanes are decision boundaries that help classify the data points. Data points falling on either side of the hyperplane can be attributed to different classes. Our objective is to find a plane that has the maximum margin, i.e the maximum distance between data points of both classes. Maximizing the margin distance provides some reinforcement so that future data points can be classified with more confidence.\n",
    "\n",
    "\n",
    "![alt text](https://miro.medium.com/max/944/0*ecA4Ls8kBYSM5nza.jpg)\n",
    "\n",
    "## What is support vectors?\n",
    "\n",
    "Support vectors are data points that are closer to the hyperplane and influence the position and orientation of the hyperplane. Using these support vectors, we maximize the margin of the classifier. Deleting the support vectors will change the position of the hyperplane. These are the points that help us build our SVM.\n",
    "\n",
    "## Pros:\n",
    "\n",
    "1. It is effective and accurate in the higher dimensional spaces.\n",
    "\n",
    "2. Effective when the number of features are more than training samples, thus works well in small  datasets. (still prone to overfitting, if number of features is much greater than the number of samples)\n",
    "\n",
    "## Cons:\n",
    "\n",
    "1. SVMs are not very efficient computationally, thus if your dataset is very big, it takes large amount of time to process.\n",
    "\n",
    "2. Less effective on noisier datasets with overlapping classes.\n",
    "\n",
    "Reference:\n",
    "1. https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47\n",
    "\n",
    "2. https://towardsdatascience.com/support-vector-machines-svm-c9ef228\n",
    "\n",
    "### 1. Twitter dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "iERkO7lcUgRk",
    "outputId": "763c0848-9a80-41d9-f327-b057eb32f945"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.01: 0.777925\n",
      "Accuracy for C=0.05: 0.784075\n",
      "Accuracy for C=0.25: 0.78105\n",
      "Accuracy for C=0.5: 0.77915\n",
      "Accuracy for C=1: 0.774025\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "#using SVM model\n",
    "ngram_vectorizer = CountVectorizer(ngram_range=(1, 3), \n",
    "                                   stop_words='english')\n",
    "ngram_vectorizer.fit(tweet_df[\"text\"])\n",
    "X = ngram_vectorizer.transform(tweet_df[\"text\"])\n",
    "target = tweet_df[\"polarity\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, target, train_size = 0.8)\n",
    "\n",
    "for c in [0.01, 0.05, 0.25, 0.5, 1]:   \n",
    "    svm = LinearSVC(C=c)\n",
    "    svm.fit(X_train, y_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(y_test, svm.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TgAOB00ZVos0"
   },
   "source": [
    "### 2. IMDB movie review dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "R-4mCciLQsdr",
    "outputId": "6a0a883f-7217-4544-8a2c-e8ac6f23b5ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.01: 0.88432\n",
      "Accuracy for C=0.05: 0.88304\n",
      "Accuracy for C=0.25: 0.88288\n",
      "Accuracy for C=0.5: 0.88256\n",
      "Accuracy for C=1: 0.88256\n"
     ]
    }
   ],
   "source": [
    "#using SVM model\n",
    "ngram_vectorizer = CountVectorizer(ngram_range=(1, 3), \n",
    "                                   stop_words='english')\n",
    "ngram_vectorizer.fit(imdb_df[\"review\"])\n",
    "X = ngram_vectorizer.transform(imdb_df[\"review\"])\n",
    "target = imdb_df[\"label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, target, train_size = 0.75)\n",
    "\n",
    "for c in [0.01, 0.05, 0.25, 0.5, 1]:   \n",
    "    svm = LinearSVC(C=c, max_iter=3000)\n",
    "    svm.fit(X_train, y_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(y_test, svm.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4ZGbf8L3Vxom"
   },
   "source": [
    "### 3. Amazon's Products review dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "743brpXydTCt",
    "outputId": "040f2a40-91c8-4ac3-b614-2947abd2c7d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.01: 0.5032888888888889\n",
      "Accuracy for C=0.05: 0.49777777777777776\n",
      "Accuracy for C=0.25: 0.4912\n",
      "Accuracy for C=0.5: 0.4885333333333333\n",
      "Accuracy for C=1: 0.488\n"
     ]
    }
   ],
   "source": [
    "#using SVM model\n",
    "ngram_vectorizer = CountVectorizer(ngram_range=(1, 3), \n",
    "                                   stop_words='english')\n",
    "ngram_vectorizer.fit(amz_df[\"reviews\"])\n",
    "X = ngram_vectorizer.transform(amz_df[\"reviews\"])\n",
    "target = amz_df[\"ratings\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, target, train_size = 0.75)\n",
    "\n",
    "for c in [0.01, 0.05, 0.25, 0.5, 1]:    \n",
    "    svm = LinearSVC(C=c, max_iter=5000)\n",
    "    svm.fit(X_train, y_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(y_test, svm.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ARqGNmH1bNFo"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# Deep Learning Method\n",
    "Deep learning is an AI function that imitates the workings of the human brain in processing data and creating patterns for use in decision making.\n",
    "\n",
    "Deep learning, a subset of machine learning, utilizes artificial neural networks to carry out the process of machine learning. The artificial neural networks are built like the human brain, with neuron nodes connected together like a web. While traditional programs build analysis with data in a linear way, the hierarchical function of deep learning systems enables machines to process data with a nonlinear approach.\n",
    "\n",
    "Neural networks are a series of algorithms that mimic the operations of a human brain to recognize relationships between vast amounts of data. There are tons of different neural networks, such as:\n",
    "\n",
    "1. Feed Forward (FF)\n",
    "\n",
    "2. Recurrent Neural Network (RNN)\n",
    "\n",
    "3. Long Short Term Memory (LSTM) \n",
    "\n",
    "4. Transformer etc.\n",
    "\n",
    "Reference: https://www.investopedia.com/terms/d/deep-learning.asp\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## What is Recurent Neural Network (RNN)?\n",
    "\n",
    "![alt text](https://miro.medium.com/max/250/1*L38xfe59H5tAgvuIjKoWPg.png)\n",
    "\n",
    "In the figure above, we see part of the neural network, A, processing some input x_t and outputs h_t. h_t is also known as the hidden state, which preserve short term memory and allows information to be passed from one step to the next. A RNN can be thought of as multiple copies of the same network, A, each network passing a message to a successor as below:\n",
    "\n",
    "![alt text](https://miro.medium.com/max/3383/1*NKhwsOYNUT5xU7Pyf6Znhg.png)\n",
    "\n",
    "The following picture shows how usually a sequence to sequence model works using RNNs. Each word is processed separately, and the resulting sentence is generated by passing a hidden state to the decoding stage that, then, generates the output.\n",
    "\n",
    "![alt text](https://miro.medium.com/max/750/1*8GcdjBU5TAP36itWBcZ6iA.gif)\n",
    "\n",
    "## The inside of RNNs:\n",
    "\n",
    "![alt text](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-SimpleRNN.png)\n",
    "In standard RNNs, this repeating module will have a very simple structure, such as a single tanh layer.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## What is the problem with RNN?\n",
    "\n",
    "RNNs become very ineffective when the gap between the relevant information and the point where it is needed become very large. That is due to the fact that the information is passed at each step and the longer the chain is, the more probable the information is lost along the chain.\n",
    "\n",
    "For example,  let’s say that you are trying to predict the last word of the text: “I grew up in France… I speak fluent …”. Recent information suggests that the next word is probably a language, but if we want to narrow down which language, we need context of France, that is further back in the text. Thus, RNN can't predict well when the information gap is too big. \n",
    "\n",
    "![alt text](https://miro.medium.com/max/3154/1*a5EbLhyxbPR78PhiV5Esjg.png)\n",
    "\n",
    "This common problem with RNN in which RNN can't handle long term dependecies well is due to vanishing gradient. Basically it means your gradient are getting lower and lower each time you propagate back to minimise your error and update your weight. The information being passed down are lost along the chain. For detailed explaination, click [here](https://www.superdatascience.com/blogs/recurrent-neural-networks-rnn-the-vanishing-gradient-problem). The invention  of Long-Short Term Memory (LSTM) network greatly addressed to this issue faced by RNN.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## What is LSTM?\n",
    "\n",
    "When arranging one’s calendar for the day, we prioritize our appointments. If there is anything important, we can cancel some of the meetings and accommodate what is important.\n",
    "\n",
    "RNNs don’t do that. Whenever it adds new information, it transforms existing information completely by applying a function. The entire information is modified, and there is no consideration of what is important and what is not.\n",
    "\n",
    "LSTMs make small modifications to the information by multiplications and additions. With LSTMs, the information flows through a mechanism known as cell states, which preserve long term memory. In this way, LSTMs can selectively remember or forget things that are important and not so important.\n",
    "\n",
    "The structure of LSTM is as below:\n",
    "\n",
    "![alt text](https://miro.medium.com/max/1305/1*MwU5yk8f9d6IcLybvGgNxA.jpeg)\n",
    "\n",
    "## How LSTM works?\n",
    "\n",
    "Each cell takes as inputs x_t (a word in the case of a sentence to sentence translation), the previous cell state c_t-1 and the output of the previous cell h_t-1. It manipulates these inputs and based on them through 3 gates (forget gate, input gate and output gate) which function differently. Afterwards, it generates a new cell state c_t, and an output h_t to pass to the next cell.\n",
    "\n",
    "![alt text](https://drive.google.com/uc?id=14TPKg6sIShWKzKSuXPl1s33s-Ufk86wB)\n",
    "\n",
    "To have futher insight on how it performs in action, we will look at ULMFit model which utilise the LSTM network.\n",
    "\n",
    "Reference: \n",
    "\n",
    "1. https://towardsdatascience.com/transformers-141e32e69591\n",
    "\n",
    "2. http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6cZKwl26jXVL"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# 1. ULMFit\n",
    "\n",
    "The idea of using generative pretrained LM + task-specific fine-tuning was first explored in ULMFiT (Howard & Ruder, 2018), the base model is AWD-LSTM. \n",
    "\n",
    "\n",
    "## ULMFit Model Architecture\n",
    "\n",
    "The following techniques are applied from the ULMFit paper to achieve better accuracy:\n",
    "\n",
    "### 1. Discriminative fine-tuning\n",
    "\n",
    "Motivated by the fact that different layers of LM capture different types of information. ULMFiT proposed to tune each layer with different learning rates, {η1,…,ηℓ,…,ηL}, where η is the base learning rate for the first layer, ηℓ is for the ℓ-th layer and there are L layers in total.\n",
    "\n",
    "\n",
    "### 2. 1-cycle learning rate policy\n",
    "\n",
    "Allows a large initial learning rate (LR_{max}=10^{-3}, for example), but decreases it by several orders of magnitude just at the last epoch. This seems to provide greater final accuracy. In the ULMFit implementation, this 1-cycle policy has been tweaked and is referred to as slanted triangular learning rate.\n",
    "\n",
    "\n",
    "### 3. Gradual unfreezing\n",
    "\n",
    "Rather than training all the layers at once during classification, the layers are \"frozen\" and the last layer is fine-tuned first, followed by the next layer before it, and so on. This avoids the phenomenon known as catastrophic forgetting (by fine-tuning all layers too aggressively).\n",
    "\n",
    "\n",
    "### 4. Concatenated pooling\n",
    "\n",
    "Because an input text can consist of hundreds or thousands of words, information might get lost if we only consider the last hidden state.\n",
    "\n",
    "Hence, the hidden state at the last time step, h_T is concatenated with both the max-pooled and mean-pooled representation of the hidden states over as many time steps as can fit in GPU memory.\n",
    "\n",
    "h_C = [h_T, maxpool(H), meanpool(H)]\n",
    "\n",
    "Where H is the vector of all hidden states.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## 3-stage fine-tuning methodology\n",
    "The classification task is done in a 3-stage process:\n",
    "\n",
    "### 1. General-domain LM pretraining: \n",
    "ULMFit has a pretrained model generated using an AWD-LSTM to develop a language model called Wikitext-103 and was trained of 28,595 preprocessed Wikipedia articles, totalling to 103 million words.\n",
    "\n",
    "### 2. Target task LM fine-tuning: \n",
    "Since the target data which we are trying to analyse, will likely come from a different distribution, ULMFit allows us to use the pre-trained language model and fine-tune it (using the above techniques) to adapt to the different context of the target data.\n",
    "    \n",
    "### 3. Target task classifier fine-tuning: \n",
    "Once we save the updated weights from the language model fine-tuning step, we can fine-tune the classifier with gradual unfreezing and the other techniques described above to perform task-specific class prediction.\n",
    "\n",
    "Reference: \n",
    "1. https://github.com/prrao87/tweet-stance-prediction\n",
    "\n",
    "2. https://docs.fast.ai/\n",
    "\n",
    "3. https://sgugger.github.io/the-1cycle-policy.html\n",
    "\n",
    "4. https://towardsdatascience.com/finding-good-learning-rate-and-the-one-cycle-policy-7159fe1db5d6\n",
    "\n",
    "5. https://forums.fast.ai/t/determining-when-you-are-overfitting-underfitting-or-just-right/7732\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 910
    },
    "colab_type": "code",
    "id": "q5FFzhXwj88d",
    "outputId": "5d215466-5c6b-4033-ad11-79fc48c1b3e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.5.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.17.5)\n",
      "Requirement already satisfied: torch==1.4.0 in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.4.0)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (6.2.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.12.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.28.1)\n",
      "Requirement already satisfied: fastai in /usr/local/lib/python3.6/dist-packages (1.0.60)\n",
      "Requirement already satisfied: spacy>=2.0.18 in /usr/local/lib/python3.6/dist-packages (from fastai) (2.1.9)\n",
      "Requirement already satisfied: bottleneck in /usr/local/lib/python3.6/dist-packages (from fastai) (1.3.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from fastai) (20.1)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from fastai) (3.1.3)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from fastai) (6.2.2)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from fastai) (0.7)\n",
      "Requirement already satisfied: nvidia-ml-py3 in /usr/local/lib/python3.6/dist-packages (from fastai) (7.352.0)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from fastai) (0.5.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from fastai) (3.13)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from fastai) (4.6.3)\n",
      "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.6/dist-packages (from fastai) (1.17.5)\n",
      "Requirement already satisfied: numexpr in /usr/local/lib/python3.6/dist-packages (from fastai) (2.7.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from fastai) (2.21.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from fastai) (1.4.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from fastai) (0.25.3)\n",
      "Requirement already satisfied: fastprogress>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from fastai) (0.2.2)\n",
      "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from fastai) (1.4.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (1.0.1)\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (0.9.6)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (1.0.2)\n",
      "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (0.2.4)\n",
      "Requirement already satisfied: thinc<7.1.0,>=7.0.8 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (7.0.8)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (0.6.0)\n",
      "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (2.0.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (2.0.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->fastai) (2.4.6)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->fastai) (1.12.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fastai) (1.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fastai) (2.6.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fastai) (0.10.0)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->fastai) (1.24.3)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->fastai) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->fastai) (2019.11.28)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->fastai) (3.0.4)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->fastai) (2018.9)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<7.1.0,>=7.0.8->spacy>=2.0.18->fastai) (4.28.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib->fastai) (45.2.0)\n",
      "Requirement already satisfied: en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0 in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
      "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
      "You can now load the model via spacy.load('en')\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch\n",
    "!pip3 install torchvision\n",
    "!pip3 install tqdm\n",
    "!pip3 install fastai\n",
    "!python3 -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VOnov8m0VWtW"
   },
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.text import *\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 570
    },
    "colab_type": "code",
    "id": "0CjWJQfBxgbG",
    "outputId": "d63c4149-6917-4e88-fef1-9d011b716cde"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>type</th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25000</td>\n",
       "      <td>train</td>\n",
       "      <td>Story of a man who has unnatural feelings for ...</td>\n",
       "      <td>neg</td>\n",
       "      <td>0_3.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25001</td>\n",
       "      <td>train</td>\n",
       "      <td>Airport '77 starts as a brand new luxury 747 p...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10000_4.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25002</td>\n",
       "      <td>train</td>\n",
       "      <td>This film lacked something I couldn't put my f...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10001_4.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25003</td>\n",
       "      <td>train</td>\n",
       "      <td>Sorry everyone,,, I know this is supposed to b...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10002_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25004</td>\n",
       "      <td>train</td>\n",
       "      <td>When I was little my parents took me along to ...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10003_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>49995</td>\n",
       "      <td>train</td>\n",
       "      <td>Seeing as the vote average was pretty low, and...</td>\n",
       "      <td>pos</td>\n",
       "      <td>9998_9.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>49996</td>\n",
       "      <td>train</td>\n",
       "      <td>The plot had some wretched, unbelievable twist...</td>\n",
       "      <td>pos</td>\n",
       "      <td>9999_8.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>49997</td>\n",
       "      <td>train</td>\n",
       "      <td>I am amazed at how this movie(and most others ...</td>\n",
       "      <td>pos</td>\n",
       "      <td>999_10.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>49998</td>\n",
       "      <td>train</td>\n",
       "      <td>A Christmas Together actually came before my t...</td>\n",
       "      <td>pos</td>\n",
       "      <td>99_8.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>49999</td>\n",
       "      <td>train</td>\n",
       "      <td>Working-class romantic drama from director Mar...</td>\n",
       "      <td>pos</td>\n",
       "      <td>9_7.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0   type  ... label         file\n",
       "0           25000  train  ...   neg      0_3.txt\n",
       "1           25001  train  ...   neg  10000_4.txt\n",
       "2           25002  train  ...   neg  10001_4.txt\n",
       "3           25003  train  ...   neg  10002_1.txt\n",
       "4           25004  train  ...   neg  10003_1.txt\n",
       "...           ...    ...  ...   ...          ...\n",
       "24995       49995  train  ...   pos   9998_9.txt\n",
       "24996       49996  train  ...   pos   9999_8.txt\n",
       "24997       49997  train  ...   pos   999_10.txt\n",
       "24998       49998  train  ...   pos     99_8.txt\n",
       "24999       49999  train  ...   pos      9_7.txt\n",
       "\n",
       "[25000 rows x 5 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = Path('/content/gdrive/My Drive/Dataset')\n",
    "trainfile = 'imdb_train.csv'\n",
    "testfile = 'imdb_test.csv'\n",
    "train_orig = pd.read_csv(path/trainfile, encoding='iso-8859-1')\n",
    "train_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "colab_type": "code",
    "id": "wsYRLHaQ4iF9",
    "outputId": "28624105-c5fb-45e7-9614-969b35a87f7f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>type</th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25000</td>\n",
       "      <td>train</td>\n",
       "      <td>Story of a man who has unnatural feelings for ...</td>\n",
       "      <td>neg</td>\n",
       "      <td>0_3.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25001</td>\n",
       "      <td>train</td>\n",
       "      <td>Airport '77 starts as a brand new luxury 747 p...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10000_4.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25002</td>\n",
       "      <td>train</td>\n",
       "      <td>This film lacked something I couldn't put my f...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10001_4.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25003</td>\n",
       "      <td>train</td>\n",
       "      <td>Sorry everyone,,, I know this is supposed to b...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10002_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25004</td>\n",
       "      <td>train</td>\n",
       "      <td>When I was little my parents took me along to ...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10003_1.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0   type  ... label         file\n",
       "0       25000  train  ...   neg      0_3.txt\n",
       "1       25001  train  ...   neg  10000_4.txt\n",
       "2       25002  train  ...   neg  10001_4.txt\n",
       "3       25003  train  ...   neg  10002_1.txt\n",
       "4       25004  train  ...   neg  10003_1.txt\n",
       "\n",
       "[5 rows x 5 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_ascii(text):\n",
    "    # function to remove non-ASCII chars from data\n",
    "    return ''.join(i for i in text if ord(i) < 128)\n",
    "  \n",
    "train_orig['review'] = train_orig['review'].apply(clean_ascii)\n",
    "train_orig.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "colab_type": "code",
    "id": "cwS6JY6bzmLE",
    "outputId": "c1371002-88fa-4b8c-fd5a-f25fce142459"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>pos</td>\n",
       "      <td>Seeing as the vote average was pretty low, and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>pos</td>\n",
       "      <td>The plot had some wretched, unbelievable twist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>pos</td>\n",
       "      <td>I am amazed at how this movie(and most others ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>pos</td>\n",
       "      <td>A Christmas Together actually came before my t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>pos</td>\n",
       "      <td>Working-class romantic drama from director Mar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                             review\n",
       "24995   pos  Seeing as the vote average was pretty low, and...\n",
       "24996   pos  The plot had some wretched, unbelievable twist...\n",
       "24997   pos  I am amazed at how this movie(and most others ...\n",
       "24998   pos  A Christmas Together actually came before my t...\n",
       "24999   pos  Working-class romantic drama from director Mar..."
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.concat([train_orig['label'], train_orig['review']], axis=1)\n",
    "train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O0aSmEd84eXf"
   },
   "outputs": [],
   "source": [
    "# Write train to csv\n",
    "train.to_csv(path/'train.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "colab_type": "code",
    "id": "ngSpw62U4xbU",
    "outputId": "75ab8881-c57c-4055-81bc-eb22778cfc75"
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Language model data\n",
    "data_lm = TextLMDataBunch.from_csv(path, 'train.csv', min_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mCFiIDEO5YLF"
   },
   "outputs": [],
   "source": [
    "# Save the language and classifier model data for re-use\n",
    "data_lm.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 464
    },
    "colab_type": "code",
    "id": "98c_u0A542Fw",
    "outputId": "a465aac3-4979-42e6-e620-6dedbed31240"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>two numbers for the lottery . \\n \\n  xxmaj our star of the picture has his number and his friend his . xxmaj when he asks his friend , would he share half of the dough , should his ticket be the winning number , his friend promptly says no . xxmaj in fact , xxup h.e. double hockey sticks no ! is the way he acts about it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>still investing time in something related to this piece of sh!t is startlingly similar to waking up after a night of suicidally heavy drinking next to the heaving form of a still slumbering 200 pound college girl . xxmaj your first urge is a desperate desire to flee . xxmaj this is natural . xxbos i imagine when xxmaj hitchcock scholars and experts find themselves together , the talk is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>cliche at all the right moments . xxmaj superb xxmaj new xxmaj york xxmaj city locations - gritty , real - are a fantastic antidote to the commercial imperatives of \" xxmaj sex in the xxmaj city \" - in fact , the entire film is an antidote to the xxup hbo / xxmaj hollywood notion of xxmaj new xxmaj york xxmaj city , sex and relationships . xxmaj it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>stooge counterparts a running gag throughout the 53- minute movie is xxmaj moe hitting xxmaj curly . xxmaj wayne 's character , a skirt chasing bully , is not very endearing , but is supposed to be the good guy . \\n \\n  xxmaj playing a traveling rodeo cowboy xxmaj wayne holds up the rodeo box office at gunpoint and takes the prize money he would have won if</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>king xxmaj david } , xxmaj susan xxmaj hayward ( xxmaj bathsheba ) , xxmaj raymond xxmaj massey ( xxmaj nathan ) , xxmaj kieron xxmaj moore ( xxmaj uriah ) and xxmaj jayne xxmaj meadows ( xxmaj michal ) . \\n \\n  xxmaj the film is based around the second xxmaj old xxmaj testament book of xxmaj samuel from the xxmaj holy xxmaj bible . xxmaj it follows</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_lm.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zHChoBHH5xAQ"
   },
   "outputs": [],
   "source": [
    "learn = language_model_learner(data_lm, arch = AWD_LSTM, pretrained = True, drop_mult=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 454
    },
    "colab_type": "code",
    "id": "NO_OYTQA50tT",
    "outputId": "f81c809b-97d0-436e-a970-7741f4e7a0c2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AWD_LSTM(\n",
       "   (encoder): Embedding(60000, 400, padding_idx=1)\n",
       "   (encoder_dp): EmbeddingDropout(\n",
       "     (emb): Embedding(60000, 400, padding_idx=1)\n",
       "   )\n",
       "   (rnns): ModuleList(\n",
       "     (0): WeightDropout(\n",
       "       (module): LSTM(400, 1152, batch_first=True)\n",
       "     )\n",
       "     (1): WeightDropout(\n",
       "       (module): LSTM(1152, 1152, batch_first=True)\n",
       "     )\n",
       "     (2): WeightDropout(\n",
       "       (module): LSTM(1152, 400, batch_first=True)\n",
       "     )\n",
       "   )\n",
       "   (input_dp): RNNDropout()\n",
       "   (hidden_dps): ModuleList(\n",
       "     (0): RNNDropout()\n",
       "     (1): RNNDropout()\n",
       "     (2): RNNDropout()\n",
       "   )\n",
       " ), LinearDecoder(\n",
       "   (decoder): Linear(in_features=400, out_features=60000, bias=True)\n",
       "   (output_dp): RNNDropout()\n",
       " )]"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(learn.model.children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 372
    },
    "colab_type": "code",
    "id": "6lUfEg0B59gb",
    "outputId": "dae31e29-66fa-43c0-a2d8-524654187dec"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='1', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/1 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='99' class='' max='1347', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      7.35% [99/1347 00:23<04:51 12.2639]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nnIJARKTOKQhEROrc/weN0D+kLH+VSQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 77
    },
    "colab_type": "code",
    "id": "sHkiI0nB75Xv",
    "outputId": "3f6c40ab-b662-4ea8-f7be-4a94313c3019"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.251603</td>\n",
       "      <td>4.017988</td>\n",
       "      <td>0.293578</td>\n",
       "      <td>05:40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn = language_model_learner(data_lm,  arch = AWD_LSTM, pretrained = True, drop_mult=0.5)\n",
    "learn.fit_one_cycle(cyc_len=1, max_lr=1e-2, moms=(0.95, 0.85))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 343
    },
    "colab_type": "code",
    "id": "_7afM4FY8Yvc",
    "outputId": "2a6de0a0-3084-423d-c299-7de0fa7eb7b7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.001808</td>\n",
       "      <td>3.941348</td>\n",
       "      <td>0.301745</td>\n",
       "      <td>05:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.128095</td>\n",
       "      <td>4.060712</td>\n",
       "      <td>0.289842</td>\n",
       "      <td>05:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.214559</td>\n",
       "      <td>4.133401</td>\n",
       "      <td>0.282311</td>\n",
       "      <td>05:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.171031</td>\n",
       "      <td>4.105160</td>\n",
       "      <td>0.285697</td>\n",
       "      <td>05:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.107690</td>\n",
       "      <td>4.054721</td>\n",
       "      <td>0.290646</td>\n",
       "      <td>05:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.952753</td>\n",
       "      <td>3.991771</td>\n",
       "      <td>0.297385</td>\n",
       "      <td>05:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.815827</td>\n",
       "      <td>3.928691</td>\n",
       "      <td>0.304834</td>\n",
       "      <td>05:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.655516</td>\n",
       "      <td>3.889913</td>\n",
       "      <td>0.310102</td>\n",
       "      <td>05:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.498987</td>\n",
       "      <td>3.881652</td>\n",
       "      <td>0.312519</td>\n",
       "      <td>05:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.386326</td>\n",
       "      <td>3.888191</td>\n",
       "      <td>0.312559</td>\n",
       "      <td>05:42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(cyc_len=10, max_lr=1e-2, moms=(0.95, 0.85))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RwwZu4GjNTgj"
   },
   "outputs": [],
   "source": [
    "# Save the fine-tuned encoder\n",
    "learn.save_encoder('ft_enc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "colab_type": "code",
    "id": "gjXtKrqwH5PT",
    "outputId": "ea44c263-8fd5-48cb-a25e-cc925d0e5bf7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Classifier model data\n",
    "data_clas = TextClasDataBunch.from_csv(path, 'train.csv', vocab=data_lm.train_ds.vocab,\n",
    "                                       min_freq=1, bs=32)\n",
    "data_clas.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BM1Gurh-IAgy"
   },
   "outputs": [],
   "source": [
    "learn = text_classifier_learner(data_clas, arch= AWD_LSTM, drop_mult=0.5)\n",
    "learn.load_encoder('ft_enc')\n",
    "learn.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 375
    },
    "colab_type": "code",
    "id": "EiqhoDXXIHau",
    "outputId": "08315ad9-c726-4e0b-d4cb-df293a920075"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='1', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/1 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='95' class='' max='624', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      15.22% [95/624 00:32<03:03 1.4106]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEJCAYAAACOr7BbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSxDGGGP8sgRhjDHGr/8HnWVtHQNybnoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 77
    },
    "colab_type": "code",
    "id": "bK9QxdlIILOz",
    "outputId": "d643101c-8fe1-4b4f-fd53-9d1e192e33d9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.314346</td>\n",
       "      <td>0.229890</td>\n",
       "      <td>0.911200</td>\n",
       "      <td>04:21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(1, max_lr=1e-2, moms=(0.95, 0.85))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 77
    },
    "colab_type": "code",
    "id": "ltFoLS7IIMBd",
    "outputId": "5c9ca106-0900-4fb1-d432-eeb5392eeeb7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.304500</td>\n",
       "      <td>0.196820</td>\n",
       "      <td>0.920200</td>\n",
       "      <td>04:54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.freeze_to(-2) \n",
    "learn.fit_one_cycle(1, slice(1e-3,1e-2), moms=(0.95,0.85))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 77
    },
    "colab_type": "code",
    "id": "poUIW7V6IPz2",
    "outputId": "556b2dc2-5a2f-4066-9a47-221f984b0d97"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.248752</td>\n",
       "      <td>0.181083</td>\n",
       "      <td>0.930800</td>\n",
       "      <td>04:23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.freeze_to(-3) \n",
    "learn.fit_one_cycle(1, slice(8e-4,1e-2), moms=(0.95,0.85))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 166
    },
    "colab_type": "code",
    "id": "gm6ZPICSIVft",
    "outputId": "c2146105-ed6f-45c2-d285-bd9fc9a90bd2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.246294</td>\n",
       "      <td>0.183238</td>\n",
       "      <td>0.927400</td>\n",
       "      <td>04:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.210732</td>\n",
       "      <td>0.163227</td>\n",
       "      <td>0.933000</td>\n",
       "      <td>04:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.139298</td>\n",
       "      <td>0.155351</td>\n",
       "      <td>0.939800</td>\n",
       "      <td>04:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.089309</td>\n",
       "      <td>0.167842</td>\n",
       "      <td>0.944600</td>\n",
       "      <td>03:53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(4, slice(8e-4,2e-2), moms=(0.95,0.85))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "colab_type": "code",
    "id": "5DO3sQmpId69",
    "outputId": "7bcc2696-1cda-40cd-969b-337e00991745"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Once again Mr. Costner has dragged out a movie...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is an example of why the majority of acti...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>First of all I hate those moronic rappers, who...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Not even the Beatles could write songs everyon...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Brass pictures (movies is not a fitting word f...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>A gritty Australian film, with all the element...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>This movie gives you more of an idiea how Aust...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>Before I start my review here is a quick lesso...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>A good story, well-acted with unexpected chara...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>Two Hands restored my faith in Aussie films. I...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 review label\n",
       "0     Once again Mr. Costner has dragged out a movie...   neg\n",
       "1     This is an example of why the majority of acti...   neg\n",
       "2     First of all I hate those moronic rappers, who...   neg\n",
       "3     Not even the Beatles could write songs everyon...   neg\n",
       "4     Brass pictures (movies is not a fitting word f...   neg\n",
       "...                                                 ...   ...\n",
       "9995  A gritty Australian film, with all the element...   pos\n",
       "9996  This movie gives you more of an idiea how Aust...   pos\n",
       "9997  Before I start my review here is a quick lesso...   pos\n",
       "9998  A good story, well-acted with unexpected chara...   pos\n",
       "9999  Two Hands restored my faith in Aussie films. I...   pos\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv(path/testfile, encoding='iso-8859-1')\n",
    "test_pred = test.filter(['review','label'])\n",
    "test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "colab_type": "code",
    "id": "Q9X9gCJgmHJy",
    "outputId": "1915d027-2b60-4a8b-c7eb-af5099526fdb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "      <th>predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Once again Mr. Costner has dragged out a movie...</td>\n",
       "      <td>neg</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is an example of why the majority of acti...</td>\n",
       "      <td>neg</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>First of all I hate those moronic rappers, who...</td>\n",
       "      <td>neg</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Not even the Beatles could write songs everyon...</td>\n",
       "      <td>neg</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Brass pictures (movies is not a fitting word f...</td>\n",
       "      <td>neg</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review label predicted label\n",
       "0  Once again Mr. Costner has dragged out a movie...   neg             neg\n",
       "1  This is an example of why the majority of acti...   neg             neg\n",
       "2  First of all I hate those moronic rappers, who...   neg             neg\n",
       "3  Not even the Beatles could write songs everyon...   neg             neg\n",
       "4  Brass pictures (movies is not a fitting word f...   neg             pos"
      ]
     },
     "execution_count": 39,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred['predicted label'] = test_pred['review'].apply(lambda row: str(learn.predict(row)[0]))\n",
    "test_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "-xTZNEZ5jg-x",
    "outputId": "b7a7050d-1dd0-43e3-8be2-92db4900b660"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score      : 0.9318844954220745\n",
      "Accuracy score: 0.9323\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score,accuracy_score\n",
    "m1 = f1_score(test_pred['label'], test_pred['predicted label'], pos_label='pos')\n",
    "m2 = accuracy_score(test_pred['label'], test_pred['predicted label'])\n",
    "print('F1 score      :',m1)\n",
    "print('Accuracy score:',m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8arFR-ybbBLT"
   },
   "source": [
    "We can see that by utilising LSTM network in ULMFit, the accuracy score improves significantly from 0.88 to 0.93. But then, there are still some rooms for improvement and problems do exist in LSTM network too.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## What is the problem with LSTM?\n",
    "\n",
    "The same problem that happens to RNNs generally, happen with LSTMs, i.e. when sentences are way too long LSTMs still don’t do well. The reason for that is that the probability of keeping the context from a word that is far away from the current word being processed decreases exponentially with the distance from it.That means that when sentences are long, the model often forgets the content of distant positions in the sequence. \n",
    "\n",
    "Another problem with RNNs, and LSTMs, is that it’s hard to parallelize the work for processing sentences, since you are have to process word by word. This results in training the model takes longer time and can't perform multiple actions simultaneously. Not only that but there is no explicit model of long and short range dependencies. \n",
    "\n",
    "Moving on, we will look at **attention** and **Convolutional Neural Networks (CNN)** on how they address and solve the issues faced by LSTMs and introducing the latest state-of-the-art neural network, **Transformers**! \n",
    "\n",
    "Reference: https://towardsdatascience.com/the-fall-of-rnn-lstm-2d1594c74ce0\n",
    " \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## What is attention?\n",
    "\n",
    "When translating a sentence, I pay special attention to the word I’m presently translating. When I’m transcribing an audio recording, I listen carefully to the segment I’m actively writing down. These are the examples of usage of attention in different scenario.\n",
    "\n",
    "Neural networks can achieve this same behavior using attention, focusing on part of a subset of the information they are given. For RNNs, instead of only encoding the whole sentence in a hidden state, each word which has a corresponding hidden state is passed all the way to the decoding stage. Then, the hidden states are used at each step of the RNN to decode. The following video shows how that happens.\n",
    "\n",
    "http://jalammar.github.io/images/seq2seq_7.mp4\n",
    "\n",
    "The idea behind it is that there might be relevant information in every word in a sentence. So in order for the decoding to be precise, it needs to take into account every word of the input, using attention. \n",
    "\n",
    "Besides, the decoder does not directly use the hidden state provided by all encoders as input, but adopts a selection mechanism to select the hidden state that best matches the current position. To do so, it tries to determine which hidden state is most closely related to the current node by calculating the score value of each hidden state and doing a softmax calculation over the scores, which allows the higher correlation of the hidden state to have a larger fractional value, and the less relevant hidden state has a lower fractional value. It then multiples each hidden state by its softmaxed score, thus amplifying hidden states with high scores, and drowning out hidden states with low scores. This scoring exercise is done at each time step on the decoder side. Kindly refer to following videos on how it is done:\n",
    "\n",
    "http://jalammar.github.io/images/attention_process.mp4\n",
    "\n",
    "http://jalammar.github.io/images/attention_tensor_dance.mp4\n",
    "\n",
    "Looking for more in depth explaination, kindly refer to http://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/. However, processing inputs (words) in parallel is still not possible. For a large corpus of text, this increases the time spent on processing the text. As result, Convolutional Neural Network is introduced to address this issue.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Why Convolutional Neural Network (CNN)?\n",
    "\n",
    "CNN enables parallelisation and thus greatly reduce the time spent in processing the inputs. Some of the most popular neural networks for sequence transduction, Wavenet and Bytenet, are CNN.\n",
    "\n",
    "![alt text](https://miro.medium.com/max/713/1*www46FWqJCc3OZQKP_QRoQ.gif)\n",
    "\n",
    "The reason why Convolutional Neural Networks can work in parallel, is that each word on the input can be processed at the same time and does not necessarily depend on the previous words to be translated. \n",
    "\n",
    "However, the problem is that CNN do not necessarily help with the problem of figuring out the problem of dependencies when translating sentences. That’s why **Transformers** were created, they are a combination of both CNN with attention.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Transformers\n",
    "\n",
    "Transformers try to solve the problems faced by LSTM by introducing CNN (for parallelisation purpose) together with attention models. Attention boosts the speed of how fast the model can translate from one sequence to another, more specifically, it uses self-attention (to be even more specific, it uses multihead attention). \n",
    "\n",
    "![alt text](https://miro.medium.com/max/1523/1*V2435M1u0tiSOz4nRBfl4g.png)\n",
    "\n",
    "The Transformer consists of six encoders and six decoders. Each encoder and decoder are similar to other encoder and decoder. Encoder consists of two layers: Self-attention and a Feed Forward Neural Network.\n",
    "\n",
    "![alt text](https://miro.medium.com/max/990/1*HaGTuYfNHWg45GZbTBnVSA.png)\n",
    "![alt text](https://miro.medium.com/max/509/1*QcTbVCVPj4WFnqvvWU5-hQ.png)\n",
    "\n",
    "The encoder’s inputs first flow through a self-attention layer. It helps the encoder look at other words in the input sentence as it encodes a specific word. The decoder has both those layers, but between them is an encoder-decoder attention layer that helps the decoder focus on relevant parts of the input sentence.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## What is Self-Attention?\n",
    "\n",
    "Self-attention is similar to attention, they fundamentally share the same concept and many common mathematical operations.  In layman’s terms, the self-attention mechanism allows the inputs to interact with each other (“self”) and find out who they should pay more attention to (“attention”). The outputs are aggregates of these interactions and attention scores. Here is how it looks like in a sentence:\n",
    "\n",
    "![alt text](https://miro.medium.com/max/1580/1*GQzYZuAMWr3lN_IACBfvAA.png)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "## How Self-Attention works?\n",
    "\n",
    "As is the case in NLP applications in general, we begin by turning each input word into a vector using an embedding algorithm. \n",
    "\n",
    "Each word is embedded into a vector of size 512 (can be changed). We’ll represent those vectors with these simple boxes.\n",
    "\n",
    "![alt text](https://miro.medium.com/max/1030/0*0oTRj6MKAYEs_cT1.png)\n",
    "\n",
    "In the bottom encoder that would be the word embeddings, but in other encoders, it would be the output of the encoder that’s directly below. After embedding the words in our input sequence, each of them flows through each of the two layers of the encoder. One key property of the Transformer, which is that the word in each position flows through its own path in the encoder. There are dependencies between these paths in the self-attention layer. The feed-forward layer does not have those dependencies, however, and thus the various paths can be executed in parallel while flowing through the feed-forward layer.\n",
    "\n",
    "![alt text](https://miro.medium.com/max/1353/0*FVCP6TqLPQeWPZqt.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Steps in calculating self-attention:\n",
    "\n",
    "Self-attention can be generalised to compute a weighted sum of the values dependent on the query and the corresponding keys.\n",
    "The query determines which values to focus on; we can say that the query ‘attends’ to the values. For example, when you type a query to search for some video on Youtube, the search engine will map your query against a set of keys (video title, description etc.) associated with candidate videos in the database, then present you the best matched videos (values).\n",
    "\n",
    "![alt text](https://miro.medium.com/max/1094/0*-P9BdUe2FCSAIpxC.png)\n",
    "\n",
    "1. Create three vectors from each of the encoder’s input vectors which are Query vector, Key vector and Value vector. These vectors are abstract vectors that extract different components of an input words and are created by multiplying the embedding by three matrices (the weight matrix) that we trained during the training process.\n",
    "\n",
    "\n",
    "2. Calculate a score to determine how much focus to place on other parts of the input sentence as we encode a word at a certain position. Score is calculated by taking the dot product of the query vector with the key vector of the respective word we’re scoring. So if we’re processing the self-attention for the word in position #1, the first score would be the dot product of q1 and k1. The second score would be the dot product of q1 and k2.\n",
    "\n",
    "![alt text](https://miro.medium.com/max/856/0*KlFsyIDK3O54l14X.png)\n",
    "\n",
    "\n",
    "3.  Divide the scores by 8 (the square root of the dimension of the key vectors used here — 64. This leads to having more stable gradients. There could be other possible values here, but this is the default).\n",
    "\n",
    "\n",
    "4. Pass the result through a softmax operation. Softmax normalizes the scores so they’re all positive and add up to 1. Softmax score determines how much how much each word will be expressed at this position. Clearly the word at this position will have the highest softmax score, but sometimes it’s useful to attend to another word that is relevant to the current word.\n",
    "\n",
    "![alt text](https://miro.medium.com/max/1084/0*rqWSBLDcJcbMmGs2.png)\n",
    "\n",
    "\n",
    "5. Multiply each value vector by the softmax score. The intuition here is to keep intact the values of the word(s) we want to focus on, and drown-out irrelevant words (by multiplying them by tiny numbers like 0.001, for example).\n",
    "\n",
    "\n",
    "6. Sum up the weighted value vectors. This produces the output of the self-attention layer at this position (for the first word). The resulting vector is one we can send along to the feed-forward neural network.\n",
    "\n",
    "![alt text](https://miro.medium.com/max/983/0*ih2c_llIiOD1-aJN.png)\n",
    "\n",
    "In the actual implementation, however, this calculation is done in matrix form for faster processing.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Multihead attention\n",
    "\n",
    "To make transformer even better, Transformers use the concept of Multihead attention. It expands the model’s ability to focus on different positions.  Yes, in the example above, z1 contains a little bit of every other encoding, but it could be dominated by the the actual word itself. It would be useful if we’re translating a sentence like “The animal didn’t cross the street because it was too tired”, we would want to know which word “it” refers to.\n",
    "\n",
    "The idea behind it is that whenever you are translating a word, you may pay different attention to each word based on the type of question that you are asking. The images below show what that means. For example, whenever you are translating “kicked” in the sentence “I kicked the ball”, you may ask “Who kicked”. Depending on the answer, the translation of the word to another language can change. Or ask other questions, like “Did what?”\n",
    "\n",
    "![alt text](https://miro.medium.com/max/1540/1*8H6TqcfHrtNCc9_Qva7xog.png)\n",
    "\n",
    "\n",
    "## All-in-one visual showing how multihad attention works:\n",
    "\n",
    "\n",
    "![alt text](http://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Positional Encoding\n",
    "\n",
    "Another important step on the Transformer is to add positional encoding when encoding each word. Positional encoders are vector that gives context based on position of word in sentence. These vectors follow a specific pattern that the model learns, which helps it determine the position of each word, or the distance between different words in the sequence. The intuition here is that adding these values to the embeddings provides meaningful distances between the embedding vectors once they’re projected into Q/K/V vectors and during dot-product attention.\n",
    "\n",
    "![alt text](http://jalammar.github.io/images/t/transformer_positional_encoding_vectors.png)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Residual Connection and Layer Normalization\n",
    "\n",
    "Each sub-layer (self-attention, ffnn) in each encoder is followed by a layer-normalization step and has a residual connection around it.\n",
    "\n",
    "![alt text](http://jalammar.github.io/images/t/transformer_resideual_layer_norm.png)\n",
    "\n",
    "Normalization helps with the problem called internal covariate shift. Internal covariate shift refers to covariate shift occurring within a neural network, i.e. going from (say) layer 2 to layer 3. This happens because, as the network learns and the weights are updated, the distribution of outputs of a specific layer in the network changes. This forces the higher layers to adapt to that drift, which slows down learning. After normalizing the input in the neural network, we don’t have to worry about the scale of input features being extremely different.\n",
    "\n",
    "To understand layer normalization, it is useful to contrast it with batch normalization. A mini-batch consists of multiple examples with the same number of features. Mini-batches are matrices — or tensors if each input is multi-dimensional — where one axis corresponds to the batch and the other axis — or axes — correspond to the feature dimensions. Batch normalization normalizes the input features across the batch dimension. The key feature of layer normalization is that it normalizes the inputs across the features. In batch normalization, the statistics are computed across the batch and are the same for each example in the batch. In contrast, in layer normalization, the statistics are computed across each feature and are independent of other examples. Layer normalization is better for the purpose of stabilization.\n",
    "\n",
    "![alt text](https://miro.medium.com/max/1280/1*hex7_me89ax78PCv2zLTzA.png)\n",
    "\n",
    "Besides, skip connections or residual connections are used to allow gradients to flow through a network directly, without passing through non-linear activation functions. Non-linear activation functions (eg: sigmoid and tanh), by nature of being non-linear, cause the gradients to explode or vanish (depending on the weights). Skip connections form conceptually a ‘bus’ which flows right the way through the network, and in reverse, the gradients can flow backwards along it too.\n",
    "\n",
    "If we’re to visualize the vectors and the layer-norm operation associated with self attention, it would look like this:\n",
    "\n",
    "![alt text](http://jalammar.github.io/images/t/transformer_resideual_layer_norm_2.png)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## The Decoder Side\n",
    "\n",
    "Now that we’ve covered most of the concepts on the encoder side, we basically know how the components of decoders work as well. But let’s take a look at how they work together.\n",
    "\n",
    "![alt text](https://miro.medium.com/max/509/1*QcTbVCVPj4WFnqvvWU5-hQ.png)\n",
    "\n",
    "The encoder start by processing the input sequence. The output of the top encoder is then transformed into a set of attention vectors K and V. These are to be used by each decoder in its “encoder-decoder attention” layer which helps the decoder focus on appropriate places in the input sequence:\n",
    "\n",
    "![alt text](http://jalammar.github.io/images/t/transformer_decoding_1.gif)\n",
    "\n",
    "The following steps repeat the process until a special symbol is reached indicating the transformer decoder has completed its output. The output of each step is fed to the bottom decoder in the next time step, and the decoders bubble up their decoding results just like the encoders did. And just like we did with the encoder inputs, we embed and add positional encoding to those decoder inputs to indicate the position of each word.\n",
    "\n",
    "![alt text](http://jalammar.github.io/images/t/transformer_decoding_2.gif)\n",
    "\n",
    "The self attention layers in the decoder operate in a slightly different way than the one in the encoder:\n",
    "\n",
    "There is a masked multi-head attention at the bottom. Masked represents a mask that masks certain values so that they do not have an effect when the parameters are updated. There are two kinds of masks in the Transformer model — padding mask and sequence mask. The padding mask is used in all the scaled dot-product attention (query.key), and the sequence mask is only used in the decoder’s self-attention.\n",
    "\n",
    "A padding mask solves the problem of input sequences being of variable length. Specifically, we pad 0 after a shorter sequence. But if the input sequence is too long, the content on the left is intercepted and the excess is discarded directly. Because the location of these fills (zeroes) is actually meaningless, our attention mechanism should not focus on these locations, so we need to do some processing. The specific approach is to add a very large negative number (negative infinity) to the values of these positions, so that the probability of these positions will be close to 0 after softmax! The padding mask is actually a tensor, each value is a Boolean, and the value of False is where we want to process.\n",
    "\n",
    "A sequence mask is designed to ensure that the decoder is unable to see future information. That is, for a sequence, at time_step t, our decoded output should only depend on the output before t, not the output after t. This is specific to the Transformer architecture because we do not have RNNs where we can input our sequence sequentially. Here, we input everything together and if there were no mask, the multi-head attention would consider the whole decoder input sequence at each position. We achieve this by generating an upper triangular matrix with the values of the upper triangles all zero and applying this matrix to each sequence.\n",
    "\n",
    "The “Encoder-Decoder Attention” layer works just like multiheaded self-attention, except it creates its Queries matrix from the layer below it, and takes the Keys and Values matrix from the output of the encoder stack\n",
    "\n",
    "Another detail is that the the decoder input will be shifted to the right by one position. One reason to do this is that we do not want our model to learn how to copy our decoder input during training, but we want to learn that given the encoder sequence and a particular decoder sequence, which has been already seen by the model, we predict the next word/character. If we don’t shift the decoder sequence, the model learns to simply ‘copy’ the decoder input, since the target word/character for position i would be the word/character i in the decoder input. Thus, by shifting the decoder input by one position, our model needs to predict the target word/character for position i having only seen the word/characters 1, …, i-1 in the decoder sequence. This prevents our model from learning the copy/paste task. We fill the first position of the decoder input with a start-of-sentence token, since that place would otherwise be empty because of the right-shift. Similarly, we append an end-of-sentence token to the decoder input sequence to mark the end of that sequence and it is also appended to the target output sentence.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## The Final Linear and Softmax Layer\n",
    "\n",
    "The decoder stack outputs a vector of floats. How do we turn that into a word? That’s the job of the final Linear layer which is followed by a Softmax Layer.\n",
    "\n",
    "The Linear layer is a simple fully connected neural network that projects the vector produced by the stack of decoders, into a much, much larger vector called a logits vector.\n",
    "\n",
    "Let’s assume that our model knows 10,000 unique English words (our model’s “output vocabulary”) that it’s learned from its training dataset. This would make the logits vector 10,000 cells wide – each cell corresponding to the score of a unique word. That is how we interpret the output of the model followed by the Linear layer.\n",
    "\n",
    "The softmax layer then turns those scores into probabilities (all positive, all add up to 1.0). The cell with the highest probability is chosen, and the word associated with it is produced as the output for this time step.\n",
    "\n",
    "![alt text](http://jalammar.github.io/images/t/transformer_decoder_output_softmax.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## The model architecture of Transformer\n",
    "\n",
    "Thus, after going through all the components in details, here is how a transformer looks like:\n",
    "\n",
    "![alt_text](https://miro.medium.com/max/1800/1*BHzGVskWGS_3jEcYYi6miQ.png)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Recap Of Training  \n",
    "\n",
    "Now that we’ve covered the entire forward-pass process through a trained Transformer, it would be useful to glance at the intuition of training the model.\n",
    "\n",
    "During training, an untrained model would go through the exact same forward pass. But since we are training it on a labeled training dataset, we can compare its output with the actual correct output.\n",
    "\n",
    "To visualize this, let’s assume our output vocabulary only contains six words(“a”, “am”, “i”, “thanks”, “student”, and “<eos>” (short for ‘end of sentence’)).\n",
    "    \n",
    "![alt text](http://jalammar.github.io/images/t/vocabulary.png)\n",
    "\n",
    "Once we define our output vocabulary, we can use a vector of the same width to indicate each word in our vocabulary. This also known as one-hot encoding. So for example, we can indicate the word “am” using the following vector:\n",
    "\n",
    "![alt_text](http://jalammar.github.io/images/t/one-hot-vocabulary-example.png)\n",
    "\n",
    "Following this recap, let’s discuss the model’s loss function – the metric we are optimizing during the training phase to lead up to a trained and hopefully amazingly accurate model.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## The Loss Function\n",
    "\n",
    "Say we are training our model. Say it’s our first step in the training phase, and we’re training it on a simple example – translating “merci” into “thanks”.\n",
    "\n",
    "What this means, is that we want the output to be a probability distribution indicating the word “thanks”. But since this model is not yet trained, that’s unlikely to happen just yet.\n",
    "\n",
    "![alt_text](http://jalammar.github.io/images/t/transformer_logits_output_and_label.png)\n",
    "\n",
    "Since the model's parameters (weights) are all initialized randomly, the (untrained) model produces a probability distribution with arbitrary values for each cell/word. We can compare it with the actual output, then tweak all the model's weights using backpropagation to make the output closer to the desired output.\n",
    "\n",
    "But note that this is an oversimplified example. More realistically, we’ll use a sentence longer than one word. For example – input: “je suis étudiant” and expected output: “i am a student”. What this really means, is that we want our model to successively output probability distributions where:\n",
    "\n",
    "a. Each probability distribution is represented by a vector of width vocab_size (6 in our toy example, but more realistically a number like 3,000 or 10,000)\n",
    "\n",
    "b. The first probability distribution has the highest probability at the cell associated with the word “i”\n",
    "\n",
    "c. The second probability distribution has the highest probability at the cell associated with the word “am”\n",
    "\n",
    "d. And so on, until the fifth output distribution indicates ‘<end of sentence>’ symbol, which also has a cell associated with it from the 10,000 element vocabulary.\n",
    "\n",
    "![alt text](http://jalammar.github.io/images/t/output_target_probability_distributions.png)\n",
    "\n",
    "After training the model for enough time on a large enough dataset, we would hope the produced probability distributions would look like this:\n",
    "\n",
    "![alt text](http://jalammar.github.io/images/t/output_trained_model_probability_distributions.png)\n",
    "\n",
    "Now, because the model produces the outputs one at a time, we can assume that the model is selecting the word with the highest probability from that probability distribution and throwing away the rest. That’s one way to do it (called greedy decoding). \n",
    "\n",
    "Another way to do it would be to hold on to, say, the top two words with highest probabilities (say, ‘I’ and ‘a’ for example), then in the next step, run the model twice: once assuming the first output position was the word ‘I’, and another time assuming the first output position was the word ‘a’, and whichever version produced less error considering both positions #1 and #2 is kept. We repeat this for positions #2 and #3…etc. This method is called “beam search”, where in our example, beam_size was two (because we compared the results after calculating the beams for positions #1 and #2), and top_beams is also two (since we kept two words). These are both hyperparameters that you can experiment with.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Reference: \n",
    "1. https://towardsdatascience.com/transformers-141e32e69591\n",
    "\n",
    "2. http://jalammar.github.io/illustrated-transformer/\n",
    "\n",
    "3. http://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/\n",
    "\n",
    "4. https://towardsdatascience.com/breaking-bert-down-430461f60efb\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0e99tmZwTl-I"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# 2. Bidirectional Encoder Representations from Transformers (BERT)\n",
    "\n",
    "BERT’s key technical innovation is applying the bidirectional training of Transformer, a popular attention model, to language modelling. A language model which is bidirectionally trained can have a deeper sense of language context and flow than single-direction language models.\n",
    "\n",
    "## How BERT is built?\n",
    "\n",
    "BERT is basically a trained Transformer Encoder stack. BERT makes use of Transformer, an attention mechanism that learns contextual relations between words (or sub-words) in a text. In its vanilla form,Transformer includes two separate mechanisms — an encoder that reads the text input and a decoder that produces a prediction for the task. Since BERT’s goal is to generate a language model, only the encoder mechanism is necessary. \n",
    "\n",
    "As opposed to directional models, which read the text input sequentially (left-to-right or right-to-left), the Transformer encoder reads the entire sequence of words at once. Therefore it is considered bidirectional, though it would be more accurate to say that it’s non-directional. This characteristic allows the model to learn the context of a word based on all of its surroundings (left and right of the word).\n",
    "\n",
    "To overcome the challenge of bidirectional in which it allows each word to indirectly see itself in a multi-layered context, BERT uses two training strategies:\n",
    "\n",
    "### 1. Masked Language Model (MLM)\n",
    "    \n",
    "Language Modeling is the task of predicting the next word given a sequence of words. In masked language modeling instead of predicting every next token, a percentage of input tokens is masked at random and only those masked tokens are predicted.\n",
    "    \n",
    "Why? Bi-directional models are more powerful than uni-directional language models. But in a multi-layered model bi-directional models do not work because the lower layers leak information and allow a token to see itself in later layers.\n",
    "    \n",
    "   The masked words are not always replaced with the masked token – [MASK] because then the masked tokens would never be seen before fine-tuning. Therefore, 15% of the tokens are chosen at random and \n",
    "\n",
    "   a. 80% of the time tokens are actually replaced with the token [MASK].\n",
    "    \n",
    "   b. 10% of the time tokens are replaced with a random token.\n",
    "    \n",
    "   c. 10% of the time tokens are left unchanged. \n",
    "    \n",
    "The model then attempts to predict the original value of the masked words, based on the context provided by the other, non-masked, words in the sequence. In technical terms, the prediction of the output words requires:\n",
    "\n",
    "   a. Adding a classification layer on top of the encoder output.\n",
    "    \n",
    "   b. Multiplying the output vectors by the embedding matrix, transforming them into the vocabulary dimension.\n",
    "    \n",
    "   c. Calculating the probability of each word in the vocabulary with softmax.\n",
    "\n",
    "![alt text](https://miro.medium.com/max/1095/0*ViwaI3Vvbnd-CJSQ.png)\n",
    "\n",
    "### 2. Next Sentence Prediction (NSP)\n",
    "\n",
    "In the BERT training process, the model receives pairs of sentences as input and learns to predict if the second sentence in the pair is the subsequent sentence in the original document. During training, 50% of the inputs are a pair in which the second sentence is the subsequent sentence in the original document, while in the other 50% a random sentence from the corpus is chosen as the second sentence. The assumption is that the random sentence will be disconnected from the first sentence.\n",
    "    \n",
    "This task can be easily generated from any monolingual corpus. It is helpful because many downstream tasks such as Question and Answering and Natural Language Inference require an understanding of the relationship between two sentences.\n",
    "\n",
    "To help the model distinguish between the two sentences in training, the input is processed in the following way before entering the model:\n",
    "\n",
    "   a. A [CLS] token is inserted at the beginning of the first sentence and a [SEP] token is inserted at the end of each sentence.\n",
    "\n",
    "   b. A sentence embedding indicating Sentence A or Sentence B is added to each token. Sentence embeddings are similar in concept to token embeddings with a vocabulary of 2.\n",
    "\n",
    "   c. A positional embedding is added to each token to indicate its position in the sequence.\n",
    "\n",
    "![alt text](https://miro.medium.com/max/1468/0*m_kXt3uqZH9e7H4w.png)\n",
    "\n",
    "To predict if the second sentence is indeed connected to the first, the following steps are performed:\n",
    "\n",
    "a. The entire input sequence goes through the Transformer model.\n",
    "\n",
    "b. The output of the [CLS] token is transformed into a 2×1 shaped vector, using a simple classification layer (learned matrices of weights and biases).\n",
    "  \n",
    "c. Calculating the probability of IsNextSequence with softmax.\n",
    "\n",
    "When training the BERT model, Masked LM and Next Sentence Prediction are trained together, with the goal of minimizing the combined loss function of the two strategies.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Some Facts about BERT\n",
    "\n",
    "1. WHAT IS THE MAXIMUM SEQUENCE LENGTH OF THE INPUT?\n",
    "\n",
    "    512 tokens.\n",
    "\n",
    "\n",
    "2. HOW MANY LAYERS ARE FROZEN IN THE FINE-TUNING STEP?\n",
    "    \n",
    "    No layers are frozen during fine-tuning. All the pre-trained layers along with the task-specific parameters are trained simultaneously.\n",
    "\n",
    "\n",
    "3. IS DISCRIMINATIVE FINE-TUNING USED?\n",
    "    \n",
    "    No. All the parameters are tuned with the same learning rate.\n",
    "\n",
    "\n",
    "4. WHAT ARE THE OPTIMAL VALUES OF THE HYPERPARAMETERS USED IN FINE-TUNING?\n",
    "\n",
    "    The optimal hyperparameter values are task-specific. But, the authors found that the following range of values works well across all tasks –\n",
    "\n",
    "    Dropout – 0.1\n",
    "    Batch Size – 16, 32\n",
    "    Learning Rate (Adam) – 5e-5, 3e-5, 2e-5\n",
    "    Number of epochs – 3, 4\n",
    "    \n",
    "    The authors also observed that large datasets (> 100k labeled samples) are less sensitive to hyperparameter choice than smaller datasets.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Steps in utilising BERT for text classification problem:\n",
    "\n",
    "1. Tokenization\n",
    "\n",
    "    a. Use the BERT tokenizer to first split the word into tokens. \n",
    "    \n",
    "    b. add the special tokens needed for sentence classifications (these are [CLS] at the first position, and [SEP] at the end of the sentence). \n",
    "\n",
    "    c. Replace each token with its id from the embedding table which is a component we get with the trained model.\n",
    "\n",
    "![alt text](http://jalammar.github.io/images/distilBERT/bert-distilbert-tokenization-2-token-ids.png)\n",
    "\n",
    "2. Flow through the BERT model\n",
    "\n",
    "    a. The output would be a vector for each input token. each vector is made up of 768 numbers (floats).\n",
    "\n",
    "    b. We ignore all except the first vector (the one associated with the [CLS] token). The one vector we pass as the input to the logistic regression model. (p.s: BERT was trained using a next sentence prediction (NSP) objective using the [CLS] token as a sequence approximate. The user may use this token (the first token in a sequence built with special tokens) to get a sequence prediction rather than a token prediction. However, averaging over the sequence may yield better results than using the [CLS] token.)\n",
    "\n",
    "![alt text](http://jalammar.github.io/images/distilBERT/bert-model-calssification-output-vector-cls.png)\n",
    "\n",
    "## How to process the output of BERT?\n",
    "\n",
    "The output for BERT looks like something as below:\n",
    "\n",
    "![alt text](http://jalammar.github.io/images/distilBERT/bert-output-tensor.png)\n",
    "\n",
    "For sentence classification, we’re only interested in BERT’s output for the [CLS] token, so we select that slice of the cube and discard everything else.  We slice that 3d tensor to get the 2d tensor .\n",
    "\n",
    "![alt text](http://jalammar.github.io/images/distilBERT/bert-output-tensor-selection.png)\n",
    "\n",
    "And now features is a 2d numpy array containing the sentence embeddings of all the sentences in our dataset. Each row corresponds to a sentence in our dataset, each column corresponds to the output of a hidden unit from the feed-forward neural network at the top transformer block of the Bert/DistilBERT model. Afterwards, we use this output to train Logistic Regression.\n",
    "\n",
    "![alt text](http://jalammar.github.io/images/distilBERT/bert-output-cls-senteence-embeddings.png)\n",
    "\n",
    "\n",
    "Reference:\n",
    "1. https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270\n",
    "\n",
    "2. http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/\n",
    "\n",
    "3. https://huggingface.co/transformers/index.html\n",
    "\n",
    "4. https://yashuseth.blog/2019/06/12/bert-explained-faqs-understand-bert-working/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 638
    },
    "colab_type": "code",
    "id": "b6p8P_nRNvN5",
    "outputId": "fbad0458-4d74-45e1-b1c3-69ba9aa7cd79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/33/ffb67897a6985a7b7d8e5e7878c3628678f553634bd3836404fef06ef19b/transformers-2.5.1-py3-none-any.whl (499kB)\n",
      "\r",
      "\u001b[K     |▋                               | 10kB 16.8MB/s eta 0:00:01\r",
      "\u001b[K     |█▎                              | 20kB 1.8MB/s eta 0:00:01\r",
      "\u001b[K     |██                              | 30kB 2.6MB/s eta 0:00:01\r",
      "\u001b[K     |██▋                             | 40kB 1.7MB/s eta 0:00:01\r",
      "\u001b[K     |███▎                            | 51kB 2.1MB/s eta 0:00:01\r",
      "\u001b[K     |████                            | 61kB 2.5MB/s eta 0:00:01\r",
      "\u001b[K     |████▋                           | 71kB 2.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████▎                          | 81kB 3.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████                          | 92kB 3.6MB/s eta 0:00:01\r",
      "\u001b[K     |██████▋                         | 102kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████▏                        | 112kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████▉                        | 122kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████▌                       | 133kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▏                      | 143kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▉                      | 153kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▌                     | 163kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▏                    | 174kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▉                    | 184kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▌                   | 194kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▏                  | 204kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▉                  | 215kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▍                 | 225kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████                 | 235kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▊                | 245kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▍               | 256kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████               | 266kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▊              | 276kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▍             | 286kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████             | 296kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▊            | 307kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▍           | 317kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████           | 327kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▋          | 337kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▎         | 348kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████         | 358kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▋        | 368kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▎       | 378kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████       | 389kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▋      | 399kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▎     | 409kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████     | 419kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▋    | 430kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▏   | 440kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▉   | 450kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▌  | 460kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▏ | 471kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▉ | 481kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▌| 491kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 501kB 2.8MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
      "Collecting tokenizers==0.5.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
      "\u001b[K     |████████████████████████████████| 3.7MB 42.4MB/s \n",
      "\u001b[?25hCollecting sentencepiece\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0MB 32.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.11.15)\n",
      "Collecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
      "\u001b[K     |████████████████████████████████| 870kB 36.5MB/s \n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
      "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.14.15)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (0.15.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (2.6.1)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=9bd7870baf19a141e4c3a7b60cddc002201f0a1cf029ed4ecf8c1933f4396c31\n",
      "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
      "Successfully installed sacremoses-0.0.38 sentencepiece-0.1.85 tokenizers-0.5.2 transformers-2.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install tensorflow==2.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wxpX7jN8Nx3l"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import torch\n",
    "import transformers as ppb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "colab_type": "code",
    "id": "HhNbHSHoRfHT",
    "outputId": "ae8a4308-0980-4666-a7f6-757c409c9508"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a stirring , funny and finally transporting re...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apparently reassembled from the cutting room f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>they presume their audience wo n't sit still f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this is a visually stunning rumination on love...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jonathan parker 's bartleby should have been t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6915</th>\n",
       "      <td>painful , horrifying and oppressively tragic ,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6916</th>\n",
       "      <td>take care is nicely performed by a quintet of ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6917</th>\n",
       "      <td>the script covers huge , heavy topics in a bla...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6918</th>\n",
       "      <td>a seriously bad film with seriously warped log...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6919</th>\n",
       "      <td>a deliciously nonsensical comedy about a city ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6920 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 review  label\n",
       "0     a stirring , funny and finally transporting re...      1\n",
       "1     apparently reassembled from the cutting room f...      0\n",
       "2     they presume their audience wo n't sit still f...      0\n",
       "3     this is a visually stunning rumination on love...      1\n",
       "4     jonathan parker 's bartleby should have been t...      1\n",
       "...                                                 ...    ...\n",
       "6915  painful , horrifying and oppressively tragic ,...      1\n",
       "6916  take care is nicely performed by a quintet of ...      0\n",
       "6917  the script covers huge , heavy topics in a bla...      0\n",
       "6918  a seriously bad film with seriously warped log...      0\n",
       "6919  a deliciously nonsensical comedy about a city ...      1\n",
       "\n",
       "[6920 rows x 2 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_df = pd.read_csv('https://github.com/clairett/pytorch-sentiment-classification/raw/master/data/SST2/train.tsv', \n",
    "                      delimiter='\\t', header=None)\n",
    "imdb_df.columns=['review','label']\n",
    "imdb_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "daFAgM2iR3u2"
   },
   "outputs": [],
   "source": [
    "#BERT model:\n",
    "model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n",
    "\n",
    "# Load pretrained model/tokenizer\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "colab_type": "code",
    "id": "6E0ALlB-SGXF",
    "outputId": "631053eb-bb57-44e7-df31-ca320ad663c4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [101, 1037, 18385, 1010, 6057, 1998, 2633, 182...\n",
       "1       [101, 4593, 2128, 27241, 23931, 2013, 1996, 62...\n",
       "2       [101, 2027, 3653, 23545, 2037, 4378, 24185, 10...\n",
       "3       [101, 2023, 2003, 1037, 17453, 14726, 19379, 1...\n",
       "4       [101, 5655, 6262, 1005, 1055, 12075, 2571, 376...\n",
       "                              ...                        \n",
       "6915    [101, 9145, 1010, 7570, 18752, 14116, 1998, 28...\n",
       "6916    [101, 2202, 2729, 2003, 19957, 2864, 2011, 103...\n",
       "6917    [101, 1996, 5896, 4472, 4121, 1010, 3082, 7832...\n",
       "6918    [101, 1037, 5667, 2919, 2143, 2007, 5667, 2561...\n",
       "6919    [101, 1037, 12090, 2135, 2512, 5054, 19570, 23...\n",
       "Name: review, Length: 6920, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized = imdb_df['review'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "colab_type": "code",
    "id": "CZKzft43S2Lf",
    "outputId": "bd417b97-d891-4200-a2bf-15d5c35341be"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  101,  1037, 18385, ...,     0,     0,     0],\n",
       "       [  101,  4593,  2128, ...,     0,     0,     0],\n",
       "       [  101,  2027,  3653, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [  101,  1996,  5896, ...,     0,     0,     0],\n",
       "       [  101,  1037,  5667, ...,     0,     0,     0],\n",
       "       [  101,  1037, 12090, ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = 0\n",
    "for i in tokenized.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n",
    "padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vJ8XdN6ym7f0"
   },
   "outputs": [],
   "source": [
    " tokenized.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "toiuConHTMPI",
    "outputId": "45e3114f-7847-4c50-89c1-bf880856f17e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6920, 67)"
      ]
     },
     "execution_count": 34,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(padded).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "z8P9fOl5TQz5",
    "outputId": "7e6e1532-a5ad-4c17-9e72-d6767bbdb2bb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6920, 67)"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = np.where(padded != 0, 1, 0)\n",
    "attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x62RW-bBTVFF"
   },
   "outputs": [],
   "source": [
    "input_ids = torch.tensor(padded)  \n",
    "attention_mask = torch.tensor(attention_mask)\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states = model(input_ids, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "--vvzVr_UxFc",
    "outputId": "2bb269b5-323f-4520-b326-41e9d72db5ef"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.5566489 , -0.33129248, -0.22280605, ..., -0.2278613 ,\n",
       "         0.6319193 ,  0.24306667],\n",
       "       [-0.28789303, -0.14285529, -0.06857958, ..., -0.3169057 ,\n",
       "         0.18455243,  0.31989798],\n",
       "       [-0.18645282,  0.30229506, -0.1851117 , ..., -0.33492973,\n",
       "         0.9848733 ,  0.5297744 ],\n",
       "       ...,\n",
       "       [-0.10313716,  0.2779548 , -0.4585574 , ...,  0.06911813,\n",
       "         0.7822481 ,  0.57749933],\n",
       "       [-0.3023639 , -0.15445913, -0.18571426, ..., -0.1518047 ,\n",
       "         0.72149014,  0.09804447],\n",
       "       [ 0.07386179, -0.17063335, -0.03027165, ..., -0.12601517,\n",
       "         0.46106923,  0.20001677]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = last_hidden_states[0][:,0,:].numpy()\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aPxZTf7FUzVU"
   },
   "outputs": [],
   "source": [
    "labels = imdb_df['label']\n",
    "train_features, test_features, train_labels,  test_labels = train_test_split(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L0JtusUyU-Mb"
   },
   "outputs": [],
   "source": [
    "# parameters = {'C': np.linspace(0.0001, 100, 20)}\n",
    "# grid_search = GridSearchCV(LogisticRegression(), parameters)\n",
    "# grid_search.fit(train_features, train_labels)\n",
    "\n",
    "# print('best parameters: ', grid_search.best_params_)\n",
    "# print('best scores: ', grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "ROTkg1u5VAOf",
    "outputId": "5200bed9-7252-4fe7-d397-6e1e72a2116b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "R8_rSFCkVMEG",
    "outputId": "3686283b-24eb-430c-fbdc-4fe3087785c1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8583815028901735"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(test_features, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5j8a6_eY4O1M"
   },
   "source": [
    "---\n",
    "\n",
    "## Transformer Pipelines\n",
    "In transformers v2.3.0, **pipelines** provides a high-level, easy to use, API for doing inference over a variety of downstream-tasks, including:\n",
    "\n",
    "1. Sentence Classification (Sentiment Analysis): Indicate if the overall sentence is either positive or negative. (Binary Classification task or Logitic Regression task)\n",
    "\n",
    "2. Token Classification (Named Entity Recognition, Part-of-Speech tagging): For each sub-entities (tokens) in the input, assign them a label (Classification task).\n",
    "\n",
    "3. Question-Answering: Provided a tuple (question, context) the model should find the span of text in content answering the question.\n",
    "\n",
    "4. Mask-Filling: Suggests possible word(s) to fill the masked input with respect to the provided context.\n",
    "5. Feature Extraction: Maps the input to a higher, multi-dimensional space learned from the data.\n",
    "\n",
    "Pipelines encapsulate the overall process of every NLP process:\n",
    "\n",
    "1. Tokenization: Split the initial input into multiple sub-entities with ... properties (i.e. tokens).\n",
    "\n",
    "2. Inference: Maps every tokens into a more meaningful representation.\n",
    "\n",
    "3. Decoding: Use the above representation to generate and/or extract the final output for the underlying task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 82,
     "referenced_widgets": [
      "887586c393bb44daae9a8f8727f96eb4",
      "f935f7a21de84e48b2458a5b6cc8d0b0",
      "13350ea1e587493ebee49c66a376de50",
      "13c7bd6e16d643f5aa88d92bd9061767",
      "08d7e07dd04249749e06066f6793a86e",
      "4dcab2a594194141b0971580b80aae50",
      "8e3afd1eb1c04b8da54a44266b7b3a45",
      "ba9bc9690b2d4bad9d1f7ea13372d5d7"
     ]
    },
    "colab_type": "code",
    "id": "BCcOwrlbqoot",
    "outputId": "8217d8ff-9591-4a50-a159-0155639c76d2"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "887586c393bb44daae9a8f8727f96eb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=230, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.99974006}]"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "\n",
    "nlp = pipeline('sentiment-analysis')\n",
    "nlp('This movie was kind of boring.')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Review of Sentiment Analysis Models.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "08d7e07dd04249749e06066f6793a86e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "13350ea1e587493ebee49c66a376de50": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4dcab2a594194141b0971580b80aae50",
      "max": 230,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_08d7e07dd04249749e06066f6793a86e",
      "value": 230
     }
    },
    "13c7bd6e16d643f5aa88d92bd9061767": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ba9bc9690b2d4bad9d1f7ea13372d5d7",
      "placeholder": "​",
      "style": "IPY_MODEL_8e3afd1eb1c04b8da54a44266b7b3a45",
      "value": "100% 230/230 [00:00&lt;00:00, 7.65kB/s]"
     }
    },
    "4dcab2a594194141b0971580b80aae50": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "887586c393bb44daae9a8f8727f96eb4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_13350ea1e587493ebee49c66a376de50",
       "IPY_MODEL_13c7bd6e16d643f5aa88d92bd9061767"
      ],
      "layout": "IPY_MODEL_f935f7a21de84e48b2458a5b6cc8d0b0"
     }
    },
    "8e3afd1eb1c04b8da54a44266b7b3a45": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ba9bc9690b2d4bad9d1f7ea13372d5d7": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f935f7a21de84e48b2458a5b6cc8d0b0": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
